[{"categories":["nomad"],"content":"Nomad is an interesting alternative to Kubernetes. As authors of Nomad say, it is a suplement to Kubernetes and offers some great features. Here you can find a guide on how to create a fresh Nomad cluster with two nodes. The first node acts as a server and client, the second node acts as a client only. I know this architecture is not recommended for production purposes, but I would like to test it only. The architecture of democontainer detail-process \" The architecture of demo Both nodes are not connected to the same network they are in west Europe but different cloud providers. To fulfill the connectivity requirements between nodes, I connect them to the same network using Nebula, a scalable overlay networking tool. If all nodes are visible directly, Nebula is not necessary. ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:0:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Install Nomad and other requirements All steps in this section must be installed on all nodes. ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:1:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Install Nomad Add the HashiCorp GPG key: curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - Add the official HashiCorp Linux repository: sudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs)main\" Update and install: sudo apt-get update \u0026\u0026 sudo apt-get install nomad To verify Nomad was installed correctly, try the nomad command. nomad ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:1:1","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Install Docker Engine I would like to test Nomad with containers, so a container runtime is required to install as well. I chose Docker, the full install manual could be found on Docker Install Page, but quick steps are here: Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker’s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. Learn about nightly and test channels. echo \\ \"deb [arch=$(dpkg --print-architecture)signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:1:2","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Install CNI plugin Install CNI plugin to be able to run Docker containers in Nomad with bridge networking. More info about Docker drivers in Nomad could be found here. You can choose a version depending on your system: curl -L -o cni-plugins.tgz https://github.com/containernetworking/plugins/releases/download/v1.0.1/cni-plugins-linux-amd64-v1.0.1.tgz sudo mkdir -p /opt/cni/bin sudo tar -C /opt/cni/bin -xzf cni-plugins.tgz ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:1:3","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Setup Nomad We need to configure Nomad before could be run. As I said before, nodes are connected using Nebula, so both nodes has two network interfaces, one with public IP (a default route) and one with internal private Nebula IP (preferred for intracluster communication). ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:2:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Configure the first node The first node has the name nomad1 nad here is the node’s Nomad configuration file content (/etc/nomad.d/nomad.hcl). Full configuration options can be found at Nomad Configuration page. data_dir = \"/opt/nomad/data\" bind_addr = \"0.0.0.0\" name = \"nomad1\"# Advertise on nebula1 interface only (nomad1 has more interfaces, one with public IP and one with private IP - nebula1 interface) advertise {# Defaults to the first private IP address. http = \"172.16.88.1\" rpc = \"172.16.88.1\" serf = \"172.16.88.1\" }# Enable server on nomad1 server { enabled = true bootstrap_expect = 1 }# Enable client on nomad1 client { enabled = true servers = [\"127.0.0.1\"] }# Enable raw_exec driver plugin \"raw_exec\" { config { enabled = true } } ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:2:1","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Configure the second node The first node has name nomad2 nad here is node’s Nomad configuration file content (/etc/nomad.d/nomad.hcl): data_dir = \"/opt/nomad/data\" bind_addr = \"0.0.0.0\" name = \"nomad2\"# Advertise on nebula2 interface only (nomad2 has more interfaces, one with public IP and one with private IP - nebula1 interface) advertise {# Defaults to the first private IP address. http = \"172.16.88.99\" rpc = \"172.16.88.99\" serf = \"172.16.88.99\" }# Enable client and setup server's IP to the nomad1 IP client { enabled = true#network_interface = servers = [\"172.16.88.1\"] }# Enable raw_exec Nomad driver plugin \"raw_exec\" { config { enabled = true } }# Consul agent will be running locally consul { address = \"127.0.0.1:8500\" } ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:2:2","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Start Nomad Enable nomad.service, start it and check its status on both nodes. sudo systemctl enable nomad.service sudo systemctl start nomad.service sudo systemctl status nomad.service If everything works fine, you can visit Nomad Admin UI and check Client status - http://node_ip_or_hostname:4646/ui/clients: Nomad UInomad1-ui \" Nomad UI ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:3:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Run example application I want to run nginx Docker image and listen on client’s node port 8080, here is the full job example, just submit only: job \"http-echo\" { datacenters = [\"dc1\"] group \"echo\" { network { mode = \"bridge\" port \"http\" { static = 8080 to = 80 } } count = 2 task \"server\" { driver = \"docker\" config { image = \"nginx\" ports = [\"http\"] } } } } If everything works fine, you will see the running job in Nomad UI: Nomad UI with Jobsnomad-ui-job \" Nomad UI with Jobs Now you can visit the Nginx page on the client’s public IP and port 8080 (two instances were required, so both client nodes should listen on port 8080): Nginx running in Nomadnomad-nginx \" Nginx running in Nomad ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:4:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Install Consul Consul automates networking for simple and secure application delivery, as said by Hasicorp. I want to test an automatic service discovery when installing any job in Nomad, install and configure Consul required. ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:5:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Install Consul Install Consul on both nodes first: sudo apt-get update \u0026\u0026 sudo apt-get install consul Configure Consul on the first node (nomad1) As the architecture diagram above shows, I want to run the Consul server and Consul agent on the same machine. This is not for production purposes and this is probably the reason why I didn’t found any option how to run Consul in both modes concurrently using configuration file /etc/consul.d/consul.hcl and run as system service. So I run Cosnul from the command line, where -dev argument says, that’s ok, run Consul as server and agent concurrently. # temporary solution, for testing only sudo consul agent -dev -client 0.0.0.0 -bind 172.16.88.1 \u0026 The Consul has UI as well: Consul UIconsul-ui \" Consul UI Configure Consul on the second node (nomad2) The second node acts as a Consul agent only, so the standard configuration could be used: sudo vi /etc/consul.d/consul.hcl I changed these line only (the full file content is in resources/consul2.hcl): bind_addr = \"172.16.88.99\" retry_join = [\"172.16.88.1\"] Enable consul.service, start it and check its status: sudo systemctl enable consul.service sudo systemctl start consul.service sudo systemctl status consul.service ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:5:1","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Run job with service discovery I changed my previous job as follow, see service section: job \"http-echo\" { datacenters = [\"dc1\"] group \"echo\" { network { mode = \"bridge\" port \"http\" { static = 8080 to = 80 } } count = 2 task \"server\" { driver = \"docker\" config { image = \"nginx\" ports = [\"http\"] } } service { name = \"http-echo\" port = \"http\" tags = [ \"macbook\", \"urlprefix-/http-echo\", ] check { type = \"http\" path = \"/\" interval = \"2s\" timeout = \"2s\" } } } } ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:6:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Test Consul DNS service One of the Consul features is its DNS feature. Consul resolves domain names to IP of Nomad’s client IP: dig @127.0.0.1 -p 8600 http-echo.service.consul SRV and the result: ; \u003c\u003c\u003e\u003e DiG 9.16.1-Ubuntu \u003c\u003c\u003e\u003e @127.0.0.1 -p 8600 http-echo.service.consul SRV ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 63772 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 5 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;http-echo.service.consul. IN SRV ;; ANSWER SECTION: http-echo.service.consul. 0 IN SRV 1 1 8080 0a000004.addr.dc1.consul. http-echo.service.consul. 0 IN SRV 1 1 8080 adf900c1.addr.dc1.consul. ;; ADDITIONAL SECTION: 0a000004.addr.dc1.consul. 0 IN A xxx.xxx.xxx.xxx server2.microk8s.node.dc1.consul. 0 IN TXT \"consul-network-segment=\" adf900c1.addr.dc1.consul. 0 IN A yyy.yyy.yyy.yyy server1.node.dc1.consul. 0 IN TXT \"consul-network-segment=\" Our testing http-echo job is visible in Consul UI as well: Consul UI Job Detailconsul-ui-job \" Consul UI Job Detail ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:6:1","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["nomad"],"content":"Resources https://www.nomadproject.io/docs/nomad-vs-kubernetes/supplement https://www.nomadproject.io/docs/nomad-vs-kubernetes/alternative https://www.hashicorp.com/c2m https://www.nomadproject.io/docs/nomad-vs-kubernetes https://www.nomadproject.io/docs/job-specification https://www.nomadproject.io/docs/configuration https://www.consul.io/docs/discovery/dns ","date":"2021-12-12","objectID":"/two-node-nomad-cluster/:7:0","tags":["kubernetes","nomad","docker","nebula"],"title":"Creating two node Nomad cluster","uri":"/two-node-nomad-cluster/"},{"categories":["network"],"content":"Nebula Network Nebula is a scalable overlay networking tool with a focus on performance, simplicity, and security. It lets you seamlessly connect computers anywhere in the world. This post is about extending the local network by another server running anywhere in the world, everything secured by the Nebula network. Devices on the local network should be able to access devices on the Nebula network and some devices need to access devices on the local network as well. ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:0:0","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Network and devices specification At first, let’s define network addresses and existing devices: Here, you can find the network address ranges: Network name Network CIDR local network 172.16.100.0/24 nebula network 172.16.88.0/24 Here, you can find the device list with IP addresses in both networks: Device Name Local Network IP Nebula IP local server 172.16.100.10 172.16.88.4 cloud server - 172.16.88.1 local PC 172.16.100.159 - local service 172.16.100.184 - ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:1:0","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Setup local server Enable packet forwarding between interfaces: echo 1 \u003e /proc/sys/net/ipv4/ip_forward ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:2:0","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Setup iptables Check a default FORWARD policy sudo iptables -L -v -n | grep \"Chain FORWARD\" If the previous command shows policy ACCEPT, you only need to add POSTROUTING rule: sudo iptables -t nat -A POSTROUTING -o nebula1 -j MASQUERADE Tip You can change the default FORWARD policy using this command (from ACCEPT to DENY): sudo iptables --policy FORWARD DROP If the default FORWARD policy is DENY, you need to enable forward traffic between nebula interface (nebula1 on my machine) and interface with IP assigned from my local network range (enp3s0 in my case) sudo iptables -A FORWARD -i nebula1 -o enp3s0 -j ACCEPT sudo iptables -A FORWARD -i enp3s0 -o nebula1 -m state --state ESTABLISHED,RELATED -j ACCEPT sudo iptables -t nat -A POSTROUTING -o enp3s0 -j MASQUERADE The previous commands enable any traffic from the cloud server to any IP in a local network. To enable specific traffic from cloud server only, run instead of the first command (sudo iptables -A FORWARD -i nebula1 -o enp3s0 -j ACCEPT) # If you want to enable to call port 80 on IP 172.16.100.184 only sudo iptables -A FORWARD -i nebula1 -p tcp -o enp3s0 -d 172.16.100.184 --dport 80 -j ACCEPT ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:2:1","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Generate Nebula certificate When generating Nebula certificate for local server, you need include -subnets argument with your local network CIDR, for example: ./nebula-cert sign -name \"local-server\" -ip \"172.16.88.4/24\" -groups \"servers,home\" -subnets \"172.16.100.0/2 4\" ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:2:2","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Setup local network Devices on the local network need to know, who is the gateway to the Nebula network. You can add a special route to each device: sudo ip route add 172.16.88.0/24 via 172.16.100.10 [dev {DEVICE}] or for example, in the case of Mikrotik device, add it globally for all devices on the local network: /ip route add comment=\"Destination nebula\" distance=1 dst-address=172.16.88.0/24 gatewa 172.16.100.10 ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:3:0","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Setup cloud server Cloud server must have a custom route to send traffic to IP CIDR 172.16.100.0/24 via nebula host 172.16.88.4, so add these lines to your Nebula config on the cloud server: tun:unsafe_routes:- route:172.16.100.0/24via:172.16.88.4This configuration adds a route on nebula startup, check it (ip route show): ip route show outputroute show \" ip route show output ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:4:0","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Test it ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:5:0","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Test access from local PC to cloud server pavel@MacBook-Pro ~ ping 172.16.88.1 PING 172.16.88.1 (172.16.88.1): 56 data bytes 64 bytes from 172.16.88.1: icmp_seq=5 ttl=64 time=63.424 ms 64 bytes from 172.16.88.1: icmp_seq=6 ttl=64 time=63.076 ms 64 bytes from 172.16.88.1: icmp_seq=7 ttl=64 time=56.294 ms 64 bytes from 172.16.88.1: icmp_seq=8 ttl=64 time=72.705 ms 64 bytes from 172.16.88.1: icmp_seq=9 ttl=64 time=65.074 ms ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:5:1","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["network"],"content":"Test access from cloud server The cloud server needs to have access to the local service available on 172.16.100.184:80: pavel@server1:~$ wget http://172.16.100.184/ --2021-12-12 21:18:24-- http://172.16.100.184/ Connecting to 172.16.100.184:80... connected. HTTP request sent, awaiting response... 200 OK ","date":"2021-12-12","objectID":"/extend-local-network-to-cloud-with-nebula/:5:2","tags":["nebula","cloud"],"title":"Extend local network to cloud with Nebula","uri":"/extend-local-network-to-cloud-with-nebula/"},{"categories":["azure"],"content":"Introduction This simple demo shows possible integration between two systems (system A and system B) using Azure Functions. The architecture constraints: All updates from the system A must be transfered into the system B The system A is listening on HTTP with REST API The system B is listening on HTTP with REST API The system B is not fully compatible in message definitions, so field mapping must be used The mapping must be saved in DB (I chose CosmosDB) Due to missing push notification in the system A, its API must be periodically checked The system B may be occasionally offline, so some type of persistent bus should be used. Overall architectureOverall architecture \" Overall architecture ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:1:0","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Deploy Azure Functions and related resources Setup environments: source .envrc ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:2:0","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Azure Function deployment # Create a required storage account for application code az storage account create --name $STORAGE_ACCOUNT_NAME --sku Standard_LRS --resource-group $RG --location $LOCATION # Create function app az functionapp create --consumption-plan-location $LOCATION \\ --runtime python \\ --runtime-version 3.8 \\ --functions-version 3 \\ --name $FUNC_APP_NAME \\ --os-type linux \\ --resource-group $RG \\ --storage-account $STORAGE_ACCOUNT_NAME ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:2:1","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Deploy Azure Service Bus # Create service bus namespace az servicebus namespace create --resource-group $RG --name $SERVICEBUS_NAMESPACE --location $LOCATION --sku Standard # Subscription model will be used, so create topic az servicebus topic create --name system-a-in \\ --resource-group $RG \\ --namespace-name $SERVICEBUS_NAMESPACE # Create a required subscription az servicebus topic subscription create --resource-group $RG --namespace-name $SERVICEBUS_NAMESPACE --topic-name system-a-in --name system-a-in-subscription ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:2:2","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Deploy and setup Cosmos DB az cosmosdb create \\ --name $COSMOSDB_ACCOUNT_NAME \\ --kind GlobalDocumentDB \\ --resource-group $RG Create Database and Collection in the Cosmos DB: # Get Key COSMOSDB_KEY=$(az cosmosdb keys list --name $COSMOSDB_ACCOUNT_NAME --resource-group $RG --output tsv |awk '{print $1}') # Create Database az cosmosdb database create \\ --name $COSMOSDB_ACCOUNT_NAME \\ --db-name $DATABASE_NAME \\ --key $COSMOSDB_KEY \\ --resource-group $RG # Create a container for customers (System B) # Create a container with a partition key and provision 400 RU/s throughput. COLLECTION_NAME=\"customers\" az cosmosdb collection create \\ --resource-group $RG \\ --collection-name $COLLECTION_NAME \\ --name $COSMOSDB_ACCOUNT_NAME \\ --db-name $DATABASE_NAME \\ --partition-key-path /id \\ --throughput 400 # Create a container for mapping COLLECTION_NAME=\"mapping\" az cosmosdb collection create \\ --resource-group $RG \\ --collection-name $COLLECTION_NAME \\ --name $COSMOSDB_ACCOUNT_NAME \\ --db-name $DATABASE_NAME \\ --partition-key-path /id \\ --throughput 400 Insert the following json into mapping container: { \"id\": \"SYSTEMB\", \"new_fields\": [ { \"input_fields\": [ \"first_name\", \"last_name\" ], \"separator\": \" \", \"output_fields\": [ \"full_name\" ], \"operation\": \"CONCAT\" }, { \"input_fields\": [ \"address_line2\" ], \"separator\": \"/\", \"output_fields\": [ \"city\", \"postal_code\" ], \"operation\": \"SPLIT\" } ], \"delete_fields\": [ \"first_name\", \"last_name\" ] } ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:2:3","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Deploy functions itself func azure functionapp publish $FUNC_APP_NAME Basic overview of deployed functions: Functions in skl-func-app: bus-to-system-b - [serviceBusTrigger] system-a - [httpTrigger] Invoke url: https://skl-func-app.azurewebsites.net/api/system-a system-b - [httpTrigger] Invoke url: https://skl-func-app.azurewebsites.net/api/system-b system-a-to-bus - [timerTrigger] ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:3:0","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Debugging and testing ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:4:0","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Running functions locally func start When all functions starts, two endpoint will be created: Functions: system-a: [GET,POST] http://localhost:7071/api/system-a system-b: [POST] http://localhost:7071/api/system-b bus-to-system-b: serviceBusTrigger system-a-to-bus: timerTrigger URL http://localhost:7071/api/system-a simulates the REST API of the System A and http://localhost:7071/api/system-b simulates the REST API of the Bystem B. ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:4:1","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Logs Log stream in console is not currently supported in Linux Consumption Apps. func azure functionapp logstream $FUNC_APP_NAME --browser ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:4:2","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Cleanup all resources When testing finished, all resources could be deleted using this command: az group delete --name $RG ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:5:0","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"Useful resources Manage your function app Azure Functions developer guide Quickstart: Create a Python function in Azure from the command line Azure Service Bus output binding for Azure Functions azure-functions-python-samples Azure Functions Python developer guide Azure Cosmos DB input binding for Azure Functions 2.x and higher Azure Functions HTTP trigger Code and test Azure Functions locally Live Metrics Stream: Monitor \u0026 Diagnose with 1-second latency Improve throughput performance of Python apps in Azure Functions ","date":"2021-10-22","objectID":"/azure-functions-system-integration/:6:0","tags":["cloud","azure","functions","cosmosdb","service-bus"],"title":"Transfer data between two systems using Azure functions","uri":"/azure-functions-system-integration/"},{"categories":["azure"],"content":"You can find here the guide: how to create an Azure Red Hat OpenShift 4 (ARO) cluster, how to setup connectivity to the new ARO cluster, how to deploy an example application. Note The full example with all resources could be found here on GitHub. ","date":"2021-09-23","objectID":"/iaac-aro/:0:0","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Create Azure Red Hat OpenShift 4 (ARO) cluster ","date":"2021-09-23","objectID":"/iaac-aro/:1:0","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Prepare environment Set the correct subsription: az account set -s TestingSubscription The file .env contains variables used in many places here to avoid typo bugs: source .envrc Create a new resource group: az group create \\ --name $RESOURCEGROUP \\ --location $LOCATION To by able to deploy ARO, these providers must be registeted: # Register the Microsoft.RedHatOpenShift resource provider: az provider register -n Microsoft.RedHatOpenShift --wait # Register the Microsoft.Compute resource provider: az provider register -n Microsoft.Compute --wait # Register the Microsoft.Storage resource provider: az provider register -n Microsoft.Storage --wait # Register the Microsoft.Authorization resource provider: az provider register -n Microsoft.Authorization --wait ","date":"2021-09-23","objectID":"/iaac-aro/:1:1","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Networking It will be created 2 subnets inside aro-vnet: master-subnet worker-subnet Network Name Network Address Usable Host Range Broadcast Address aro-vnet 10.0.0.0/22 10.0.0.1 - 10.0.3.254 10.0.3.255 master-subnet 10.0.0.0/23 10.0.0.1 - 10.0.1.254 10.0.1.255 worker-subnet 10.0.2.0/23 10.0.2.1 - 10.0.3.254 10.0.3.255 The ARO requires a special subnet for worker and master nodes, so create a new vnet first: az network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 10.0.0.0/22 Create subnet for master nodes: az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create subnet for worker nodes: az network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable subnet private endpoint policies on the master subnet. This is required for the service to be able to connect to and manage the cluster: az network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true ","date":"2021-09-23","objectID":"/iaac-aro/:1:2","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Setup permissions Create a new service principal (AAD role owner reqiured): az ad sp create-for-rbac -n \"${PREFIX}arosp\" --skip-assignment Take appId and password and set it: APPID=\"changeit\" PASSWORD=\"changeit\" Tip More info about creating service principals: https://docs.microsoft.com/en-us/azure/aks/kubernetes-service-principal#delegate-access-to-other-azure-resources Obtain the full registry ID and VNETID for adding roles: ACR_REGISTRY_ID=$(az acr show --name $ACR_NAME --query id --output tsv) VNETID=$(az network vnet show -g $RESOURCEGROUP --name aro-vnet --query id -o tsv) Assign role to service principal (AAD role Owner required): # Assign SP Permission to VNET az role assignment create --assignee $APPID --scope $VNETID --role \"Network Contributor\" # Assign SP Permission to ACR az role assignment create --assignee $APPID --scope $ACR_REGISTRY_ID --role acrpull Tip For more info about accessing ACR with SP visit https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auth-service-principal ","date":"2021-09-23","objectID":"/iaac-aro/:1:3","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Creating Azure Red Hat OpenShift 4 cluster All command before were just preparing for ARO deployment: az aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --client-id $APPID \\ --client-secret $PASSWORD ","date":"2021-09-23","objectID":"/iaac-aro/:1:4","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Image pull secrets Kubernetes uses an image pull secret to store information needed to authenticate to your registry. To create the pull secret for an Azure container registry, you provide the service principal ID, password, and the registry URL. Create an image pull secret with the following kubectl command: oc create secret docker-registry acr-secret \\ --namespace test \\ --docker-server=$ACR_NAME.azurecr.io \\ --docker-username=$APPID \\ --docker-password=$PASSWORD Tip Tip for copy existing secret from another namespace to default namespace: oc get secret acr-secret --namespace=test -o yaml | grep -v '^\\s*namespace:\\s' | oc apply --namespace=default -f - Once you’ve created the image pull secret, you can use it to create Kubernetes pods and deployments. Provide the name of the secret under imagePullSecrets in the deployment file. For example: apiVersion:v1kind:Podmetadata:name:my-awesome-app-podnamespace:awesomeappsspec:containers:- name:main-app-containerimage:sklenaracr.azurecr.io/my-awesome-app:v1imagePullPolicy:IfNotPresentimagePullSecrets:- name:acr-secret","date":"2021-09-23","objectID":"/iaac-aro/:1:5","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Securing ARO Create NSG and assign to aro-vnet: # Create NSG az network nsg create -g $RESOURCEGROUP -n \"aro-nsg\" # Assing it to master-subnet az network vnet subnet update -g $RESOURCEGROUP -n master-subnet --vnet-name aro-vnet --network-security-group \"aro-nsg\" # Assign it to worker-subnet az network vnet subnet update -g $RESOURCEGROUP -n worker-subnet --vnet-name aro-vnet --network-security-group \"aro-nsg\" Create NSG rule for API server: az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"apiserver_in\" --priority 101 \\ --source-address-prefixes \"$CURRENT_IP\" \\ --destination-port-ranges '6443' \\ --destination-address-prefixes \"10.0.0.0/23\" --access Allow \\ --description \"API Server IN.\" Setup inbound rules: # Take ingress ip from ARO create stdout result: #\"ingressProfiles\": [ # { # \"ip\": \"xxx.xxx.xxx.xxx\", # \"name\": \"default\", # \"visibility\": \"Public\" # } # ], INGRESS_IP=\"xxx.xxx.xxx.xxx\" #or INGRESS_IP=$(az aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"ingressProfiles[0].ip\" -o tsv) az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"ingress-443\" --priority 501 \\ --source-address-prefixes \"$CURRENT_IP\" \\ --destination-port-ranges '443' \\ --destination-address-prefixes \"$INGRESS_IP\" --access Allow \\ --description \"Ingress Allow IN.\" Setup outbound rules (All outbound traffic is enabled by default): # Allow ntp outbound az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"OUT-ALLOW-NTP\" \\ --priority 300 --source-address-prefixes \"10.0.0.0/23\" \"10.0.2.0/23\" --destination-address-prefixes \"Internet\" \\ --destination-port-ranges '123' --direction Outbound --access Allow --protocol Udp --description \"NTP\" # Allow https outbound az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"OUT-ALLOW-HTTPS\" \\ --priority 301 --source-address-prefixes \"10.0.0.0/23\" \"10.0.2.0/23\" --destination-address-prefixes \"Internet\" \\ --destination-port-ranges '443' --direction Outbound --access Allow --protocol Tcp --description \"HTTPS\" # Allow dns tcp outbound az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"OUT-ALLOW-DNS-TCP\" \\ --priority 302 --source-address-prefixes \"10.0.0.0/23\" \"10.0.2.0/23\" --destination-address-prefixes \"Internet\" \\ --destination-port-ranges '53' --direction Outbound --access Allow --protocol Tcp --description \"DNS TCP\" # Alow dns udp outbound az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"OUT-ALLOW-DNS-UDP\" \\ --priority 303 --source-address-prefixes \"10.0.0.0/23\" \"10.0.2.0/23\" --destination-address-prefixes \"Internet\" \\ --destination-port-ranges '53' --direction Outbound --access Allow --protocol Udp --description \"DNS Udp\" # Allow intra vnet communication az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"OUT-ALLOW-ARO-VNET\" --priority 400 \\ --source-address-prefixes \"10.0.0.0/23\" \"10.0.2.0/23\" \\ --destination-port-ranges '*' --direction Outbound \\ --destination-address-prefixes \"10.0.0.0/23\" \"10.0.2.0/23\" --access Allow \\ --description \"Intra vnet communication.\" # Default deny all outbound traffic az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"OUT-DENY-ALL\" --priority 4096 \\ --source-address-prefixes \"*\" \\ --destination-port-ranges '*' --direction Outbound \\ --destination-address-prefixes \"*\" --access Deny \\ --description \"Deny outbound.\" ","date":"2021-09-23","objectID":"/iaac-aro/:1:6","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Setup connectivity to ARO cluster Set connectivity to Azure: az login # follow instructions # List of subscriptions az account list -o table # Select TestingSubscription az account set -s TestingSubscription ","date":"2021-09-23","objectID":"/iaac-aro/:2:0","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Access to OpenShift Web Console source .envrc # The simple way az aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP The following example output shows what the password will be in kubeadminPassword: { \"kubeadminPassword\": \"\u003cgenerated password\u003e\", \"kubeadminUsername\": \"kubeadmin\" } You can find the cluster console URL by running the following command, which will look like this: https://console-openshift-console.apps.xxxxx.westeurope.aroapp.io/ az aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \"consoleProfile.url\" -o tsv Once you’re logged into the OpenShift Web Console, click on the ? on the top right and then on Command Line Tools. Download the release appropriate to your machine. You can download client tools here: https://console-openshift-console.apps.xxxxx.westeurope.aroapp.io/command-line-tools or directly from WEB Console: Download CLI ToolsAlt text \" Download CLI Tools ","date":"2021-09-23","objectID":"/iaac-aro/:2:1","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Connect with CLI Tools Open current user profile (right upper corner) and click on “Copy Login Command”: Copy Login CommandAlt text \" Copy Login Command Copy and paste login command to your console: Log in with this tokenAlt text \" Log in with this token Test connectivity: oc get nodes ","date":"2021-09-23","objectID":"/iaac-aro/:2:2","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Deploying an example application Deploying to OpenShift: # Create namespace oc create namespace test # Deploy apps oc apply -f example.yaml --namespace test Setting access to deployed app: # Retrieve your IP public address using curl CURRENT_IP=$(curl ifconfig.me) # Retrieve your IP public address using dig CURRENT_IP=$(dig @resolver1.opendns.com ANY myip.opendns.com +short) # or visit https://www.whatismyip.com/ and set manually: CURRENT_IP=\"xxx.xxx.xxx.xxx\" # list all running services oc get services --namespace test # Save service IP to variable SERVICE_IP=$(oc get svc voting-app --namespace test -o jsonpath='{.status.loadBalancer.ingress[*].ip}') # Create a rule to enable incoming traffic from load balancer az network nsg rule create -g $RESOURCEGROUP --nsg-name \"aro-nsg\" -n \"IN-ALLOW-WEB\" --priority 600 \\ --source-address-prefixes \"$CURRENT_IP\" \\ --destination-port-ranges '80' \\ --destination-address-prefixes \"$SERVICE_IP\" --access Allow \\ --description \"Intra test application.\" Go to: echo http://$SERVICE_IP ","date":"2021-09-23","objectID":"/iaac-aro/:3:0","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["azure"],"content":"Resources https://docs.microsoft.com/en-us/azure/openshift/tutorial-create-cluster ","date":"2021-09-23","objectID":"/iaac-aro/:4:0","tags":["azure","aro","openshift"],"title":"Creating Azure Red Hat OpenShift 4 cluster","uri":"/iaac-aro/"},{"categories":["continuous-integration","groovy"],"content":"This post is about the dynamic creating of project folders in Jenkins using Job DSL Plugin. The newly created project folders will be accessible by a specific group or user only, so you are able to use your Jenkins instance by multiple teams and each team will have their own folder for their projects. Before we can start, the following plugins must be installed: Matrix Authorization Strategy Job DSL Plugin ","date":"2020-01-01","objectID":"/jenkins-creating-dynamic-project-folders-with-job-dsl/:0:0","tags":null,"title":"Jenkins - Creating Dynamic Project Folders with Job DSL","uri":"/jenkins-creating-dynamic-project-folders-with-job-dsl/"},{"categories":["continuous-integration","groovy"],"content":"Setting Up Correct Authorization Type The first step is related to set up a correct authorization type in the Configure Global Security menu: The menu Configure Global Security Here you have to select the Project-based Matrix Authorization Strategy type and at least grant Overall/Read permission to Authenticated Users to enable login for all successfully authenticated users. Then you have to grant Administrator rights to you or any group. If you skip the setting correct authorization type, folders will be created without any problems but the authorization setting will not be applied. After that, we can create a Job DSL project. ","date":"2020-01-01","objectID":"/jenkins-creating-dynamic-project-folders-with-job-dsl/:0:1","tags":null,"title":"Jenkins - Creating Dynamic Project Folders with Job DSL","uri":"/jenkins-creating-dynamic-project-folders-with-job-dsl/"},{"categories":["continuous-integration","groovy"],"content":"Creating Job DSL project We need to create a Freestyle project (here the project-generator name): Creating a new freestyle project As our goal is about creating project folders, we create a new parameter using a checkbox “This project is parameterized”. Then we add a new String parameter PROJECT_NAME: Adding a String Parameter to project-generator job After that, the most important part is here. In the Build section, add build step and select Process Job DSLs: and paste the following groovy script into the text field: def folderName = PROJECT_NAME.toUpperCase() def adminGroup = \"ROLE_${folderName}_ADMIN\" def opsGroup = \"ROLE_${folderName}_READER\" folder(folderName) { displayName(folderName) description(\"Folder for project ${PROJECT_NAME} generated by ${JOB_NAME}\") authorization { permissions(opsGroup, [ 'hudson.model.Item.Read', 'hudson.model.Item.ViewStatus', 'hudson.model.View.Read' ]) permissionAll(adminGroup) } } When we save the job and run Build with Parameters, the job will create a new folder based on the PROJECT_NAME parameter (e.g. PROJECT1) and will be accessible by role ROLE_PROJECT1_ADMIN (as folder administrators with all permissions) and by role ROLE_PROJECT1_READER as job status readers. The project-generator job page contains a list of created project folders: List of all generated project folders When we go to the PROJECT1 folder and into its configuration, you can check assigned rights. All available rights to assign are these: com.cloudbees.plugins.credentials.CredentialsProvider.Create com.cloudbees.plugins.credentials.CredentialsProvider.Delete com.cloudbees.plugins.credentials.CredentialsProvider.ManageDomains com.cloudbees.plugins.credentials.CredentialsProvider.Update com.cloudbees.plugins.credentials.CredentialsProvider.View hudson.model.Item.Build hudson.model.Item.Cancel hudson.model.Item.Configure hudson.model.Item.Create hudson.model.Item.Delete hudson.model.Item.Discover hudson.model.Item.Move hudson.model.Item.Read hudson.model.Item.Release hudson.model.Item.ViewStatus hudson.model.Item.Workspace hudson.model.Run.Delete hudson.model.Run.Replay hudson.model.Run.Update hudson.model.View.Configure hudson.model.View.Create hudson.model.View.Delete hudson.model.View.Read hudson.scm.SCM.Tag When you try to login with user-defined in role e.g. ROLE_PROJECT1_ADMIN, you will see your folders only: ","date":"2020-01-01","objectID":"/jenkins-creating-dynamic-project-folders-with-job-dsl/:0:2","tags":null,"title":"Jenkins - Creating Dynamic Project Folders with Job DSL","uri":"/jenkins-creating-dynamic-project-folders-with-job-dsl/"},{"categories":["continuous-integration","groovy"],"content":"Integration Jenkins with Remote Systems If you need to integrate your job (i.e. project-generator) with some 3rd party systems as a final step of any approval process (e.g. in Jira), you may use Jenkins remote API to run our job. Before that, you need to generate a token to be used in the authentication during the call Jenkins API. You are able to generate a new token in the configuration page of any user: Now you can make a call with curl from the command line (replace TOKEN string by your own token, JENKINS_URL by your Jenkins URL, USER by related username and JOB_NAME by e.g. project-generator): Option 1 (using form-data): curl -X POST JENKINS_URL/job/JOB_NAME/build \\ --user USER:TOKEN \\ --form json='{\"parameter\": [{\"name\":\"PROJECT_NAME\", \"value\":\"PROJECT2\"}]}' Option 2 (using get parameters): curl -X POST JENKINS_URL/job/JOB_NAME/buildWithParameters?PROJECT_NAME=PROJECT2 \\ --user USER:TOKEN The job will be executed with the project name parameter and now your Jenkins is ready for a new project and its own CI/CD build jobs. ","date":"2020-01-01","objectID":"/jenkins-creating-dynamic-project-folders-with-job-dsl/:0:3","tags":null,"title":"Jenkins - Creating Dynamic Project Folders with Job DSL","uri":"/jenkins-creating-dynamic-project-folders-with-job-dsl/"},{"categories":null,"content":"  CodeIT is a clean, elegant but advanced blog theme for Hugo. It is based on the original LoveIt Theme, LeaveIt Theme and KeepIt Theme. Hugo Theme CodeITHugo Theme CodeIT \" Hugo Theme CodeIT ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"About CodeIT","uri":"/about/"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Plausible Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances Extended Features  Search supported by Lunr.js or algolia  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Dynamic scroll supported by Smooth Scroll  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","tags":null,"title":"About CodeIT","uri":"/about/"},{"categories":null,"content":"License CodeIT is licensed under the MIT license. Check the LICENSE file for details. Thanks to the authors of following resources included in the theme: normalize.css Font Awesome Simple Icons Animate.css Smooth Scroll autocomplete.js Lunr.js algoliasearch lazysizes object-fit-images Twemoji lightgallery.js clipboard.js Sharer.js TypeIt $ \\KaTeX $ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:0:2","tags":null,"title":"About CodeIT","uri":"/about/"},{"categories":["java","security"],"content":"Have you ever image how many vulnerabilities exist in your applications since it was created? And how many of them comes from its dependencies? The high secure environments require to perform regularly checks to discover any new vulnerability issue in your application. These checks can be done manually, but it may take a lot of time, especially if you are using the current frameworks (e.g. Spring Boot) with many and many transitive dependencies. ","date":"2019-02-27","objectID":"/vulnerability-scan-as-part-of-continuous-integration/:0:0","tags":["build","ci","jenkins","scan","spring-boot","vulnerability"],"title":"Vulnerability Scan as part of Continuous Integration","uri":"/vulnerability-scan-as-part-of-continuous-integration/"},{"categories":["java","security"],"content":"The National Vulnerability Database The National Vulnerability Database (NVM) is the U.S. government repository of standards-based vulnerabilities. Any vulnerability found in various software, hardware, databases, OS, and libraries gets its own id, e.g.: CVE-2017-5753 (Meltdown bug). Severity distribution of all found vulnerabilities over time shows the following figure: CVSS Severity Distribution Over TimeCVSS Severity Distribution Over Time \" CVSS Severity Distribution Over Time For example, there is a high chance, that your application developed in 2016 has many potential vulnerabilities through its dependencies now, although you have used the current libraries at that time. The connectivity between the NVM database and your application is provided bythe Open Web Application Security Project (OWASP, a worldwide not-for-profit charitable organization focused on improving the security of software), especially one of its project OWASP Dependency Check. It offers a various way of scanning (a command line tool, Maven or Ant plugin etc.), but we will use the Jenkins plugin to integrate vulnerability scan into continuous integration process. ","date":"2019-02-27","objectID":"/vulnerability-scan-as-part-of-continuous-integration/:1:0","tags":["build","ci","jenkins","scan","spring-boot","vulnerability"],"title":"Vulnerability Scan as part of Continuous Integration","uri":"/vulnerability-scan-as-part-of-continuous-integration/"},{"categories":["java","security"],"content":"Plugin Configuration The plugin setup is very easy. I suppose that you already have a running instance of Jenkins server with your configured project inside. If not, you can quickly run Jenkins in Docker, install Dependency Check plugin and import my example project from GitHub with a really critical bug (CVE-2017-8046) in Spring Data Rest (affected all version prior to version 2.6.9, fix released on November 27, 2017 but was in place from 2014!, just to imagine, how it is important to regularly check your projects). If you have imported your project in Jenkins, just add Invoke Dependency-Check analysis as a Post Step (for Pipeline project, check this page): Invoke Dependency-Check analysisInvoke Dependency-Check analysis settings \" Invoke Dependency-Check analysis ","date":"2019-02-27","objectID":"/vulnerability-scan-as-part-of-continuous-integration/:2:0","tags":["build","ci","jenkins","scan","spring-boot","vulnerability"],"title":"Vulnerability Scan as part of Continuous Integration","uri":"/vulnerability-scan-as-part-of-continuous-integration/"},{"categories":["java","security"],"content":"Test it No special settings needed, it scans from the root of your project for all existing libraries in it. You can optionally check to Generate optional HTML report and Generate optional vulnerability report (HTML) to generate a summary about found vulnerabilities, here is my result (generated in Workspace root): Vulnerability Report for spring-rest-data-exploitVulnerability Report for spring-rest-data-exploit \" Vulnerability Report for spring-rest-data-exploit To integrate vulnerability scan into CI process, you can add a new Post-build Action Publish Dependency-Check results: Publish Dependency-Check results settingsPublish Dependency-Check results settings \" Publish Dependency-Check results settings Here you can define your own Status thresholds, for example mark your build unstable if any high severity found in your project. On the project page, you can find some statistics about found vulnerabilities in time: Found vulnerabilities\" Found vulnerabilities It’s all. You can see, performing a regular vulnerability scan does not cost much time and may prevent many attacks. ","date":"2019-02-27","objectID":"/vulnerability-scan-as-part-of-continuous-integration/:3:0","tags":["build","ci","jenkins","scan","spring-boot","vulnerability"],"title":"Vulnerability Scan as part of Continuous Integration","uri":"/vulnerability-scan-as-part-of-continuous-integration/"},{"categories":["docker","java"],"content":"It exists many “Docker for Java developers” guides, but most of them does not take care of small and efficient Docker images. I have combined many resources how to make a simple and fast Docker image containing any of Spring Boot like application. My goals: Create a single and portable Dockerfile (as general as possible). Make Maven build inside Docker (no need to have Maven locally). Don’t download any Maven dependencies repeatedly, if no changes in pom.xml (rebuilding image as fast as possible). The final Docker image should contain only application itself (no source codes, no Maven dependencies required by Maven build etc.) The final image should be as small as possible (no full JDK required). The application inside Docker should remain configurable as much as possible (with all Spring Boot configuration options). Possibility to enable debug (on demand). Possibility to see log files. The final image is designed for development purpose, but it does not contain any no-go production parts and it is fully configurable. To see a working example, see my Github project. To fulfill a single portable Dockerfile requirement, I need to use Docker multi-stage builds. It will have two main parts (stages): The building part The runtime part ","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:0:0","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"The building part of the Dockerfile ### BUILD imageFROMmaven:3-jdk-11 as builder# create app folder for sourcesRUN mkdir -p /buildWORKDIR/buildCOPY pom.xml /build#Download all required dependencies into one layerRUN mvn -B dependency:resolve dependency:resolve-plugins#Copy source codeCOPY src /build/src# Build applicationRUN mvn packageI have started from the official Maven image, so you may change this as you wish. The most interesting part is this: RUN mvn -B dependency:resolve dependency:resolve-pluginsIt downloads all dependencies required either by your application or by plugins called during a build process. Then all dependencies are a part of one layer. That layer does not change until any changes in pom.xml found. So the rebuilding is very fast and does not include downloading all dependencies again and again. The second option how to download required dependencies comes from the official Docker Maven site (when you have some problems with the previous variant): RUN mvn -B -e -C -T 1C org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:1:0","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"How to customize Maven settings? It may exist many situations, where you need to change a default Maven setting for your customized build. To do that you need to copy your settings.xml into the image before at the start of the builder image definition, for example: FROMmaven:3-jdk-11 as builder#Copy Custom Maven settingsCOPY settings.xml /root/.m2/","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:1:1","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"The runtime part of the Dockerfile FROMopenjdk:11-slim as runtimeEXPOSE8080#Set app home folderENV APP_HOME /app#Possibility to set JVM options (https://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html)ENV JAVA_OPTS=\"\"#Create base app folderRUN mkdir $APP_HOME#Create folder to save configuration filesRUN mkdir $APP_HOME/config#Create folder with application logsRUN mkdir $APP_HOME/logVOLUME$APP_HOME/logVOLUME$APP_HOME/configWORKDIR$APP_HOME#Copy executable jar file from the builder imageCOPY --from=builder /build/target/*.jar app.jarENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar app.jar\"]#Second option using shell form:#ENTRYPOINT exec java $JAVA_OPTS -jar app.jar $0 $@The runtime part starts with some necessary step, i.e. exposing port, setting environments and creating some useful folders. The most interesting part is related to copying a previously created jar file into our new image: #Copy executable jar file from the builder imageCOPY --from=builder /build/target/*.jar app.jarI am copying from the builder image, see the param –from. For more info about copying files from other images, see the Docker documentation page for multi-stage builds. As for the Spring Boot application, the created jar file is executable, so it is possible to run our application with the single command: ENTRYPOINT [ \"sh\", \"-c\", \"java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar app.jar\" ]To reduce Tomcat startup time there is a system property pointing to “/dev/urandom”. It exists other options how to run Spring Boot application inside Docker. For more info, visit the official Spring guide. ","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:2:0","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"How to build and run Spring Boot application in Docker in one step? docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 \u003cimage_tag\u003e The above command will build your application with Maven and start it without any delay. This is the simplest way without any customizations. The life may come with some specific requirements, so here’s a couple of them. Now you can visit the URL to get response from my GitHub example: http://localhost:8081/customer/10 ","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:3:0","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"How to debug?  My example uses Java 11, so there are some JVM options to enable debug mode: docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 -p 5005:5005 --env JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 \u003cimage_tag\u003e You need to add the docker environment variable JAVA_OPTS with JVM options and map the internal debugging port to the outside of the container: -p 5005:5005. For Java 5-8 containers, use this JAVA_OPTS parameter: JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005 ","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:3:1","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"How to setup logging? The runtime container contains folder /app/log with all log files. This path could be easily mounted into your host: docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 -v /opt/spring-boot/test/log:/app/log \u003cimage_tag\u003e ","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:3:2","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["docker","java"],"content":"How to change application configuration? The jar file contains default configuration. To selectively override those values, you have many options. I will show you some a part of them. Please note that all of the configuration magics are possible when using the exec form of the ENTRYPOINT. When using the shell form of the ENTRYPOINT, you need to pass all command line arguments manually: ENTRYPOINT exec java $JAVA_OPTS -jar app.jar $0 $@Command line arguments The Spring Boot automatically accepts all command line arguments and these arguments are passed into run command inside Docker: docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 \u003cimage_tag\u003e --logging.level.org.springframework=debug System properties The similar way is using regular system properties: docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 --env JAVA_OPTS=-Dlogging.level.org.springframework=DEBUG \u003cimage_tag\u003e Environment variables You may use environment variables instead of system properties. Most operating systems disallow period-separated key names, but you can use underscores instead (for example, SPRING_CONFIG_NAME instead of spring.config.name). Check the documentation page for more information. docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 --env LOGGING_LEVEL_ORG_SPRINGFRAMEWORK=DEBUG \u003cimage_tag\u003e Mount your own configuration file You may have noticed that there is a VOLUME for mounting configuration folder: docker build -t \u003cimage_tag\u003e . \u0026\u0026 docker run -p 8080:8080 -v /opt/spring-boot/test/config:/app/config:ro \u003cimage_tag\u003e So your local folder /opt/spring-boot/test/config should contain the file application.properties. This is the default configuration file name and can be easily changed by setting the property spring.config.name. It’s all, but your requirements may vary in many ways. I tried to solve some of the most important conditions to use Docker for Java developers. Without resolving them, it was not worth talking about using Docker for Java developers. As mentioned above, see the project example on my GitHub with all of the mentioned code. Some interesting links: The official Spring Boot in Docker guide Externalized Configuration in Spring Boot Docker Multi-stage builds OpenJDK Docker images Maven Docker images ","date":"2018-12-16","objectID":"/spring-boot-run-and-build-in-docker/:3:3","tags":["docker","java","maven","spring-boot"],"title":"Spring Boot: Run and Build in Docker","uri":"/spring-boot-run-and-build-in-docker/"},{"categories":["java"],"content":"This post is about an example of securing REST API with a client certificate (a.k.a. X.509 certificate authentication). In other words, a client verifies a server according to its certificate and the server identifies that client according to a client certificate (so-called the mutual authentication). In connection with Spring Security, we will be able to perform some additional authentication and authorization. Technologies used: Spring Boot 2.0.5.RELEASE Spring Web + Security 5.0.8.RELEASE Embedded Tomcat 8.5.34 Quick post overview: Create a simple REST API service (without any security) Create certificates for server and client Configure the server to serve HTTPS content Configure the server to require a client certificate Spring Security for further client’s authentication and authorization Test secured REST API ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:0:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Final Project Structure ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:1:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Creating a new base Spring Boot project We will start with a new project generated by Spring Initializr. We need just only two Spring dependencies, i.e. Spring Web + Spring Security. All required dependencies are shown here: \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-security\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:2:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Create a simple REST API service Let’s create a simple REST controller serving a detail about a customer using HTTP GET method: @RestController @RequestMapping(\"/customer\") public class CustomerController { @GetMapping(\"/{id}\") public Customer GetCustomer(@PathVariable Long id) { return new Customer(id, \"Customer\" + id); } } Displaying URL http://localhost:8080/customer/1 returns this JSON object: { \"id\":1, \"name\":\"Customer1\" } ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:3:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Create certificates for server and client I want to stay in focus on securing REST API so I will show you how to generate all required files in a very concise way. For more details about commands, visit my other blog post about creating a PKCS #12 key store. #Create folders to generate all files (separated for client and server) mkdir ssl \u0026\u0026 cd ssl \u0026\u0026 mkdir client \u0026\u0026 mkdir server ## Server # Generate server private key and self-signed certificate in one step openssl req -x509 -newkey rsa:4096 -keyout server/serverPrivateKey.pem -out server/server.crt -days 3650 -nodes # Create PKCS12 keystore containing private key and related self-sign certificate openssl pkcs12 -export -out server/keyStore.p12 -inkey server/serverPrivateKey.pem -in server/server.crt # Generate server trust store from server certificate keytool -import -trustcacerts -alias root -file server/myCertificate.crt -keystore server/trustStore.jks ## Client # Generate client's private key and a certificate signing request (CSR) openssl req -new -newkey rsa:4096 -out client/request.csr -keyout client/myPrivateKey.pem -nodes ## Server # Sign client's CSR with server private key and a related certificate openssl x509 -req -days 360 -in request.csr -CA server/server.crt -CAkey server/serverPrivateKey.pem -CAcreateserial -out client/pavel.crt -sha256 ## Client # Verify client's certificate openssl x509 -text -noout -in client/pavel.crt # Create PKCS12 keystore containing client's private key and related self-sign certificate openssl pkcs12 -export -out client/client_pavel.p12 -inkey client/myPrivateKey.pem -in client/pavel.crt -certfile server/myCertificate.crt You can find the SSL folder with all generated files on the project’s GitHub page. We will use files in the server folder to configure our server. The final client’s file client/client_pavel.p12 can be either imported into your browser or used in another client application. On Windows just simply open this file and import it into your system to test REST API with any browser. ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:4:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Configure the server to serve HTTPS content Basically, there are two options on how to do it. You can use any standalone server (e.g. Tomcat, WildFly etc.) so the configuration would be specific to your choice. I prefer this choice for production environments. Instead of configuring an application server, I will show you the second simpler way of using embedded Tomcat server inside Spring Boot. The configuration is quite easy, we will change the port to 8443 and configure the server key store generated in the previous steps: # Define a custom port (instead of the default 8080) server.port=8443 # The format used for the keystore server.ssl.key-store-type=PKCS12 # The path to the keystore containing the certificate server.ssl.key-store=classpath:keyStore.p12 # The password used to generate the certificate server.ssl.key-store-password=changeit ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:5:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Configure the server to require a client certificate The configuration of any server to require a client certificate (i.e. the mutual authentication) is very similar to the server side configuration except using words like a trust store instead of a key store. So the embedded Tomcat configuration seems like this: # Trust store that holds SSL certificates. server.ssl.trust-store=classpath:trustStore.jks # Password used to access the trust store. server.ssl.trust-store-password=changeit # Type of the trust store. server.ssl.trust-store-type=JKS # Whether client authentication is wanted (\"want\") or needed (\"need\"). server.ssl.client-auth=need The embedded server ensures now (without any other configuration) that the clients with a valid certificate only are able to call our REST API. Other clients will be declined by the server due to unable to make correct SSL/TLS handshake (required by the mutual authentication). Please note, that all configuration items starting server.* are related to an embedded Tomcat only. You do not need it when using any standalone application server. ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:6:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Spring Security for further client authentication and authorization It would be fine to get an incoming client into our application as a logged user. It will give us the possibility to perform some other authentications and authorizations using Spring Security (e.g. securing method call to the specific role only). Until now no Spring Security was needed, but all clients with any valid certificate may perform any call in our application without knowing who is the caller. So we must configure Spring Security to create a logged user using a username from a client certificate (usually from the CN field, see the method call subjectPrincipalRegex): @Configuration @EnableWebSecurity @EnableGlobalMethodSecurity(securedEnabled = true) public class SecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http.authorizeRequests() .anyRequest().authenticated().and() .x509() .subjectPrincipalRegex(\"CN=(.*?)(?:,|$)\") .userDetailsService(userDetailsService()); } @Bean public UserDetailsService userDetailsService() { return (UserDetailsService) username -\u0026amp;amp;amp;amp;gt; { if (username.equals(\"pavel\")) { return new User(username, \"\", AuthorityUtils .commaSeparatedStringToAuthorityList(\"ROLE_USER\")); } else { throw new UsernameNotFoundException(String.format(\"User %s not found\", username)); } }; } } Using the bean UserDetailsService is a kind of fake, but it shows an example of an additional authentication to accept only username “pavel”. In other words, it accepts a client with a certificate containing value “pavel” in certificate’s field CN only (as mentioned before, configured with subjectPrincipalRegex). As you maybe noticed, only user “pavel” is the member of the role “user”, so now we are able to restrict method calls to the specific role only: @GetMapping(\"/{id}\") @Secured(\"ROLE_USER\") public Customer GetCustomer(@PathVariable Long id) { return new Customer(id, \"Customer\" + id); } ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:7:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"Test secured REST API When you successfully imported client/client_pavel.p12 into your system and the application runs, you can visit URL https://localhost:8443/customer/1. The first access of this page displays a window to select the correct certificate to authenticate with the server: Selecting certificate\" Selecting certificate When you submit a wrong certificate, you will see the “access denied” page (otherwise JSON object returned): Access denied when wrong certificate\" Access denied when wrong certificate It is all, you can find all source codes on my GitHub profile. Some useful links about this topic: Spring Boot Official: Configure SSL ","date":"2018-10-10","objectID":"/securing-rest-api-with-client-certificate/:8:0","tags":null,"title":"Securing REST APIs with Client Certificates","uri":"/securing-rest-api-with-client-certificate/"},{"categories":["java"],"content":"This post is about using Spring Shell to make a simple application for scanning open TCP ports. Technologies used: Spring Boot 2.0.5.RELEASE Spring Shell 2.0.1.RELEASE Quick Overview: Final Project Structure Creating a new base Spring Boot project Needs for parallelism How check whether a port is open? Integrating with Spring Shell How it works ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:0:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["java"],"content":"Final Project Structure The final project structure\" The final project structure ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:1:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["java"],"content":"Creating a new base Spring Boot project We will start with a new project generated by Spring Initializr. We need just only one Spring dependency, i.e. Spring Shell. All required dependencies are shown here: \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.shell\u003c/groupId\u003e \u003cartifactId\u003espring-shell-starter\u003c/artifactId\u003e \u003cversion\u003e2.0.1.RELEASE\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:2:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["java"],"content":"Needs for parallelism Scanning open ports can take a lot of time, especially when you are scanning thousands of ports. It is clear we need to parallel this task. The level of the parallelism can be defined with a number of possible threads inside our thread pool (just to be sure, we will use async support from Spring): @Configuration @EnableAsync public class AsyncConfig implements AsyncConfigurer { @Value(\"${threads.count:20}\") private int threadsCount; @Override public Executor getAsyncExecutor() { return Executors.newFixedThreadPool(threadsCount); } } ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:3:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["java"],"content":"How check whether a port is open? Each of tries to check whether a port is open will contain the same step, i.e. a connect using java.net.Socket to a specific IP address and a specific port. When a connection timeout occurs a port is not open. Take notice that our method is annotated with @Async and return type is Future. It means that everyone who calls this method will not be blocked until the end of a scanning process. In other words, this method starts running in another thread than the caller’s thread. @Async public Future\u003cScanResult\u003e checkPort(String ip, int port) { try { Socket socket = new Socket(); socket.connect(new InetSocketAddress(ip, port), timeout); socket.close(); return new AsyncResult\u003c\u003e(new ScanResult(port, true)); } catch (IOException ex) { return new AsyncResult\u003c\u003e(new ScanResult(port, false)); } } ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:4:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["java"],"content":"Integrating with Spring Shell Our goal is to create an application which will be able to scan open ports on a specific IP address. So it would be fine to create a command line application and Spring Shell will help us to do that. At first, we need to create a new command so we will create a new class. Each of a method it will be a standalone command callable from a command line. The Spring Shell just only needs to know about this class and so-called method commands. All of this can be configured using annotations: @ShellComponent on a class level and @ShellMethod on a method level. Command parameters are the same as method parameters with the possibility to customize it using @ShellOption. Here is our implementation. First of all, we create a task for each of port to scan (calling method addToScan) and then we print a result to an output. There is some magic to be able to scan either a single port or range of ports: @ShellComponent public class ScannerCommand { public static final String PORT_SEPARATOR = \"-\"; private final ScannerService scannerService; @Autowired public ScannerCommand(ScannerService scannerService) { this.scannerService = scannerService; } @ShellMethod(value = \"Scan open ports for a specific IP address\") public String scan( @ShellOption(help = \"IP address\") String ip, @ShellOption(help = \"Port or port range, e.g. 1-1024\") String port, @ShellOption(help = \"Weather only open ports should be displayed\") boolean displayOnlyOpen ) throws ExecutionException, InterruptedException { //Add all required ports into port scanner List\u003cFuture\u003cScannerService.ScanResult\u003e\u003e futureList; if (port.contains(PORT_SEPARATOR)) { String[] rangeLimits = port.split(PORT_SEPARATOR); futureList = addToScan(ip, range(Integer.parseInt(rangeLimits[0]), Integer.parseInt(rangeLimits[1]))); } else { futureList = addToScan(ip, Integer.parseInt(port)); } //Read and write results for (final Future\u003cScannerService.ScanResult\u003e scanResultFuture : futureList) { ScannerService.ScanResult scanResult = scanResultFuture.get(); if (displayOnlyOpen) { if (scanResult.isOpen()) { System.out.println(scanResult); } } else { System.out.println(scanResult); } } return \"DONE\"; } private List\u003cFuture\u003cScannerService.ScanResult\u003e\u003e addToScan(String ip, int... ports) { List\u003cFuture\u003cScannerService.ScanResult\u003e\u003e result = new ArrayList\u003c\u003e(); for (int port : ports) { result.add(scannerService.checkPort(ip, port)); } return result; } } ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:5:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["java"],"content":"How it works When you successfully run our application, you will see a command line prompt: shell:\u003e You can use some predefined command such a help with our scan command: shell:\u003e help scan NAME scan - Scan open ports for a specific IP address SYNOPSYS scan [--ip] string [--port] string [--display-only-open] OPTIONS --ip string IP address [Mandatory] --port string Port or port range, e.g. 1-1024 [Mandatory] --display-only-open Weather only open ports should be displayed [Optional, default = false] So let’s try our command with the option –display-only-open to display open ports only: shell:\u003escan --ip 10.15.13.52 --port 1-1024 --display-only-open port 22 - open port 25 - open DONE You can find all source codes on my GitHub profile. Some useful links about this topic: How To Do @Async in Spring Spring Shell Reference Documentation ","date":"2018-09-18","objectID":"/tcp-port-scanner-using-spring-shell/:6:0","tags":null,"title":"TCP Port Scanner Example using Spring Shell","uri":"/tcp-port-scanner-using-spring-shell/"},{"categories":["security"],"content":"This post is about creating PKCS #12 to serve e.g. your content via HTTPS in your application itself or in another web container (such a Tomcat or another application server). The PKCS #12 format is a binary format for storing cryptography objects. It usually contains the server certificate, any intermediate certificates (i.e. chain of trust), and the private key, all of them in a single file. A PKCS #12 file may be encrypted and signed. PKCS #12 files are usually found with the extensions .pfx and .p12. The PKCS #12 is similar to JKS format, but you can use it not only in Java but also in other libraries in C, C++ or C# etc, so I prefer this type of a keystore to be more general. To use PKCS #12 inside your application, you have two way how to do it: Create your own self-signed SSL certificate Create a certificate using the Certificate Signing Request (CSR, a.k.a PKCS #10) The first option is fast and simple, but not suitable for production environment. The second option is about creating CSR to be signed by any trusted Certificate Authority (CA). ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:0:0","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Create your own self-signed SSL certificate When you need to create a new certificate as quickly as possible, run the following two commands: ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:1:0","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Generate a private key and a certificate in separated files using PEM format openssl req -x509 -newkey rsa:4096 -keyout myPrivateKey.pem -out myCertificate.crt -days 3650 -nodes openssl – the command for executing OpenSSL. req – certificate request and certificate generating utility in OpenSSL. -x509 – used to generate a self-signed certificate. -newkey rsa:4096 - option to create a new certificate request and a new private key, rsa:4096 means generating an RSA key nbits in size. -keyout myPrivateKey.pem – use the private key file myPrivateKey.pem as the private key to combining with the certificate. -out myCertificate.crt – use myCertificate.crt as the output certificate name. -days 3650 – specifies the number of days to certify the certificate for. -nodes - a created private key will not be encrypted. ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:1:1","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Combine a private key and a certificate into one key store in the PKCS #12 format openssl pkcs12 -export -out keyStore.p12 -inkey myPrivateKey.pem -in myCertificate.crt openssl – the command for executing OpenSSL. pkcs12 – the PKCS #12 utility in OpenSSL. -export - the option specifies that a PKCS #12 file will be created. -out keyStore.p12 – specifies a filename to write the PKCS #12 file to. -inkey myPrivateKey.pem – file to read private key from. -in myCertificate.crt – the filename to read the certificate. The wizard will prompt you for an export password. If filled, this password will be used as a key store password. And that is all you need, use keyStore.p12 in your application. ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:1:2","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Create a certificate using the Certificate Signing Request ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:2:0","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Generate a private key and a certificate signing request into separated files openssl req -new -newkey rsa:4096 -out request.csr -keyout myPrivateKey.pem -nodes openssl – the command for executing OpenSSL. req – certificate request and certificate generating utility in OpenSSL. -newkey rsa:4096 - option to create a new certificate request and a new private key, rsa:4096 means generating an RSA key nbits in size. -keyout myPrivateKey.pem – use the private key file myPrivateKey.pem as the private key to combining with the certificate. -out request.csr – use request.csr as the certificate signing request in the PKCS #10 format. -nodes - a created private key will not be encrypted. ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:2:1","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Generate a certificate signing request from an existing private key openssl req -new -key myPrivateKey.pem -out request.csr openssl – the command for executing OpenSSL. req – certificate request and certificate generating utility in OpenSSL. -new - generates a new certificate request. -key myPrivateKey.pem – specifies the file to read the private key from. -out request.csr – use request.csr as the certificate signing request in the PKCS#10 format. Now it is time to send request.csr as a result of the previous step to your CA (Certificate Authority) to be signed. You are almost done. When you get a new certificate for your request.csr from your CA, use it together with a private key to create a PKCS#12 file: ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:2:2","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["security"],"content":"Combine a private key and a certificate into one key store in the PKCS #12 format openssl pkcs12 -export -out keyStore.p12 -inkey privateKey.pem -in certificate.crt -certfile CA.crt openssl – the command for executing OpenSSL. pkcs12 – the PKCS #12 utility in OpenSSL. -export - the option specifies that a PKCS #12 file will be created. -out keyStore.p12 – specifies a filename to write the PKCS #12 file to. -inkey myPrivateKey.pem – file to read private key from. -in myCertificate.crt – the filename to read the certificate. -certfile CA.crt – optional parameter to read additional certificates from, useful to create a complete trust chain. The output file keyStore.p12 is what you need to add to your application. When you filled an export password use it as a key store password in a configuration file. ","date":"2018-08-18","objectID":"/how-to-create-pkcs-12-for-your-application/:2:3","tags":null,"title":"How To Create PKCS #12 For Your Application","uri":"/how-to-create-pkcs-12-for-your-application/"},{"categories":["synology"],"content":"This post is about installing an Ubuntu desktop on your Synology NAS. You are able to install any other Linux distribution using this recipe, it does not matter whether desktop or server type of any distro. To install any virtual machine to your Synology, you need to install Virtual Machine Manager to your NAS. Your model must be supported: 19 series: RS1219+ 18 series: FS1018, RS3618xs, RS818RP+, RS818+, RS2818RP+, RS2418RP+, RS2418+, DS3018xs, DS918+, DS718+, DS218+, DS1618+ 17 series: FS3017, FS2017, RS3617xs, RS3617RPxs, RS4017xs+, RS3617xs+, RS18017xs+, DS3617xs, DS1817+, DS1517+ 16 series: RS2416RP+, RS2416+, RS18016xs+, DS916+ 15 series: RS815RP+, RS815+, RC18015xs+, DS3615xs, DS2415+, DS1815+, DS1515+ 14 series: RS3614xs, RS3614RPxs, RS3614xs+ 13 series: RS3413xs+, RS10613xs+ 12 series: RS3412xs, RS3412RPxs, DS3612xs 11 series: RS3411xs, RS3411RPxs, DS3611xs (source, last updated 2018-08-13) We can summarize a recipe into the following steps: Downloading an installation image Creating a virtual machine Installation of a system Post-installation recommendations ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:0:0","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Downloading an installation image You can install your favourite image (ISO) of your distro. I have used an Ubuntu Desktop because it is very popular with most people (actually I do not know why :)). Download ubuntu-xxx-desktop-amd64.iso and save it to your NAS. ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:1:0","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Creating Virtual Machine Creating your own Virtual Machine is the most important step, but Synology has its own wizard which is very helpful and I think is very easy to understand. So run Virtual Machine Manager, select Virtual Machine on the left menu a click to create. Create a new virtual machine\" Create a new virtual machine ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:0","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Choose Operating System In the first step of the wizard, select the Linux choice. Option Other may be helpful for some exotic or old distros. Wizard step 1\" Wizard step 1 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:1","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Select Storage The step 2 is related to storage, where your machine will physically be installed. If it is your first install, you will be prompted to create a new one. Your data on NAS will not be affected, all ubuntu files will be separated. Wizard step 2\" Wizard step 2 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:2","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"General Configuration The step 3 is related to general configuration. As you maybe noticed, you are creating a new virtual machine, so you need to assign CPU (2), RAM (3), video card (3) etc. A number of assigned RAM is related to your available memory, but you need to know, that the DSM system needs memory for its run as well so you cannot count with 4 virtual machines (4 x 1GB), when your NAS has 4 GB in total. Actually, exactly one virtual machine consumes memory assigned by you plus 256 MB + 80 MB (resources for the hypervising: Each VM requires 256 MB; each vCPU requires 80 MB), so 1GB assigned to your Virtual Machine needs 1360 MB in total. Wizard step 3\" Wizard step 3 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:3","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Storage The 4th step is related to storage. Now you need to set up your previously downloaded ISO image for bootup (1) and setup capacity of the virtual disk (2). As you noticed, I set 10 GB what is quite sufficient for showing Ubuntu (after installing it takes about 5 GB) Wizard step 4\" Wizard step 4 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:4","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Network Configuration The step number 5 is related to network configuration. If you do not have any special requirements, you can leave it as it, or you can disable network inside your Ubuntu, so no internet access. Wizard step 5\" Wizard step 5 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:5","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Other Settings The last but one step is not so important, all default choices are sufficient. Maybe you can set autostart to yes. Wizard step 6\" Wizard step 6 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:6","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Assign Permissions The last step is about assigning permissions to power on/off or restart a virtual machine, I checked admin only. Wizard step 7\" Wizard step 7 ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:7","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Summary After submitting the summary page, your virtual machine will be running (please, check Power on the virtual machine after creation). Summary\" Summary On the Virtual Machine page, select your previously created machine and click to Connect (1).A new tab with vnc session to your machine will be created. Please follow steps depending on your needs and an installing system. ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:2:8","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Installation of a system Installation system...\" Installation system... Your installation process has been completed. Installation completed\" Installation completed ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:3:0","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["synology"],"content":"Post-installation recommendations When the installation process is done, it is strongly recommended to install a guest agent. It is used to exchange information between the host and guest, and to execute a command in the guest, allowing properly shut down or restart your machine from the Virtual Machine Manager application. The command is here: sudo apt-get install qemu-guest-agent Installing qemu-guest-agent\" Installing qemu-guest-agent And it is all, enjoy your new virtual machine. ","date":"2018-08-13","objectID":"/how-to-install-ubuntu-on-synology/:4:0","tags":null,"title":"How to install Ubuntu on Synology","uri":"/how-to-install-ubuntu-on-synology/"},{"categories":["java"],"content":"In this example, we will secure a home page (/home) with Spring Security using Radius authentication. Technologies used: Spring Boot 2.0.4.RELEASE TinyRadius 1.0.1 Embedded Tomcat 8.5.23 Quick Overview: Create a new base Spring Boot project with required dependencies Create a simple login using Spring Security Create your own RadiusAuthenticationProvider Simple test with a real Radius Server ","date":"2018-01-23","objectID":"/spring-security-radius-login/:0:0","tags":["docker","radius","spring-boot"],"title":"Spring Security with Radius Login in Spring Boot","uri":"/spring-security-radius-login/"},{"categories":["java"],"content":"1. Project Structure Project structure\" Project structure ","date":"2018-01-23","objectID":"/spring-security-radius-login/:1:0","tags":["docker","radius","spring-boot"],"title":"Spring Security with Radius Login in Spring Boot","uri":"/spring-security-radius-login/"},{"categories":["java"],"content":"2. Create a new base Spring Boot project We will start with a new project generated by Spring Initializr. Just only two Spring dependencies are required: Security and Web. Spring InitializrSpring Initializr \" Spring Initializr The generated project is ready to run with embedded Tomcat, but we need to add other required dependencies The first dependency is a [tinyradius](http://tinyradius.sourceforge.net/) as a client library to be able to call Radius server. The other two dependencies (jstl and tomcat-embed-jasper) are related to JSP pages, which will be used inside a login and a home page. All required dependencies are shown here: \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-security\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ejavax.servlet\u003c/groupId\u003e \u003cartifactId\u003ejstl\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.tomcat.embed\u003c/groupId\u003e \u003cartifactId\u003etomcat-embed-jasper\u003c/artifactId\u003e \u003cscope\u003eprovided\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.hynnet\u003c/groupId\u003e \u003cartifactId\u003etinyradius\u003c/artifactId\u003e \u003cversion\u003e1.0.1\u003c/version\u003e \u003c/dependency\u003e Spring Initialzr did usually not generate any dummy pages, so we need to add our pages, login.jsp and home.jsp. We do not need an extra Controller class, so we register our pages into ViewControllerRegistry directly. ViewController is required to be able to map view to its JSP page. @Configuration public class MvcConfig extends WebMvcConfigurerAdapter { @Override public void addViewControllers(ViewControllerRegistry registry) { registry.addViewController(\"/\").setViewName(\"home\"); registry.addViewController(\"/home\").setViewName(\"home\"); registry.addViewController(\"/login\").setViewName(\"login\"); } @Bean public InternalResourceViewResolver viewResolver() { InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/pages/\"); resolver.setSuffix(\".jsp\"); return resolver; } } ","date":"2018-01-23","objectID":"/spring-security-radius-login/:2:0","tags":["docker","radius","spring-boot"],"title":"Spring Security with Radius Login in Spring Boot","uri":"/spring-security-radius-login/"},{"categories":["java"],"content":"3. Create a simple login using Spring Security It exits many examples how to configure Spring Security, so check google if you need to customize it. We do not need any special configuration, just to secure a home page. We will display a login page for non-authenticated users, it’s all, no more requirements, no roles. @Configuration @EnableWebSecurity public class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http .authorizeRequests() .anyRequest().authenticated() .and() .formLogin() .loginPage(\"/login\") .permitAll() .and() .logout() .permitAll(); } } ","date":"2018-01-23","objectID":"/spring-security-radius-login/:3:0","tags":["docker","radius","spring-boot"],"title":"Spring Security with Radius Login in Spring Boot","uri":"/spring-security-radius-login/"},{"categories":["java"],"content":"4. Create your own RadiusAuthenticationProvider You need to implement your own AuthenticationProvider, so the method authenticate is required: @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException { String username = authentication.getName(); RadiusPacket response = null; int attemptCount = 0; while (response == null \u0026\u0026 attemptCount++ \u003c clients.size()) { response = authenticateInternally(clients.get(attemptCount - 1), username, authentication.getCredentials().toString()); } if (response == null) { logger.warn(\"User {}, calling radius does not return any value.\", username); return null; } if (response.getPacketType() == RadiusPacket.ACCESS_ACCEPT) { logger.info(\"User {} successfully authenticated using radius\", username); return new UsernamePasswordAuthenticationToken(username, \"\", new ArrayList\u003c\u003e()); } else { logger.warn(\"User {}, returned response {}\", username, response); return null; } } There is nothing special in this method except working with a response from the Radius server. We accept just only RadiusPacket.ACCESS_ACCEPT which indicates, that our authentication has been successful. The communication between our application (a.k.a. network access server - NAS) and a RADIUS server is implemented in the class NetworkAccessServer: public RadiusPacket authenticate(String login, String password) throws IOException, RadiusException { AccessRequest ar = new AccessRequest(login, password); ar.setAuthProtocol(AccessRequest.AUTH_PAP); ar.addAttribute(NAS_PORT_ID, InetAddress.getLocalHost().getHostAddress()); ar.addAttribute(NAS_IP_ADDRESS, \"172.25.0.101\"); RadiusPacket response = radiusClient.authenticate(ar); return response; } I have used some commonly used parameters, e.g. NAS-IP-Address and NAS-Port-Id, but you can set any other settings depends on your server. ","date":"2018-01-23","objectID":"/spring-security-radius-login/:4:0","tags":["docker","radius","spring-boot"],"title":"Spring Security with Radius Login in Spring Boot","uri":"/spring-security-radius-login/"},{"categories":["java"],"content":"5. Simple test with a real Radius Server We have prepared our application to make a real authentication test (please check the project source code to download a fully working copy. Now we can simply start the Spring Boot web app: $mvn spring-boot:run The home page (http://localhost:8080/) is password protected, so we need to log in first: Login window\" Login window The real Radius server is required to make a real authentication test. If you do not have any free Radius server to test, you can make your test against a Radius server running inside Docker. Thanks to Docker, you are able to run a real server in one minute, check an useful tutorial here. When using pre-configured docker’s container you can simply start a fully configured Radius server with this command: $docker-compose up -d freeradius My application contains a correct configuration to call this testing Radius server as well (the 2nd server’s configuration, servers are delimited by semicolon): com.pavelsklenar.radius.server=192.168.1.1,secret,500;127.0.0.1,SECRET,1000 The source code of this project could be found on my public Github profile. To prevent any other future changes, the fully compatible Radius server with configuration files is forked on my Github profile as well. Note: To run this project in the IntelliJ Idea (in the version 2018.2.x and above) using their Spring Boot plugin, set in the Run configuration Working directory to $MODULE_WORKING_DIR$. No other changes are required. Tips to remove provided element in the pom.xml are not required anymore. Some useful links about spring Security: Spring Security Authentication Provider Spring Boot + Spring Security + Thymeleaf example Spring Security in MVC 4 Using Spring Boot ","date":"2018-01-23","objectID":"/spring-security-radius-login/:5:0","tags":["docker","radius","spring-boot"],"title":"Spring Security with Radius Login in Spring Boot","uri":"/spring-security-radius-login/"},{"categories":["java"],"content":"This example will demonstrate how to use Spring Integration for downloading files from a remote SFTP server. Two possible authentications could be used, i.e. public key or password. Technologies used: Spring Boot 2.0.4.RELEASE Spring Integration 5.0.7.RELEASE (managed by Spring Boot) Spring 5.0.8.RELEASE (managed by Spring Boot) Quick overview: Create SFTP Session Factory, i.e. DefaultSftpSessionFactory Create and set up InboundChannelAdapter to regularly check a remote SFTP server for new files Create MessageHandler to process incoming files ","date":"2017-05-22","objectID":"/spring-integration-sftp-download/:0:0","tags":null,"title":"Spring Integration: SFTP Download using Key-Based Authentication","uri":"/spring-integration-sftp-download/"},{"categories":["java"],"content":"Project Structure A final project directory structurespring-integration sftp download example \" A final project directory structure ","date":"2017-05-22","objectID":"/spring-integration-sftp-download/:1:0","tags":null,"title":"Spring Integration: SFTP Download using Key-Based Authentication","uri":"/spring-integration-sftp-download/"},{"categories":["java"],"content":"SftpConfig using Java Configuration We have to configure SFTP Session Factory (DefaultSftpSessionFactory) with all required parameters, i.e. host, IP port, username and password (or private key with a passphrase). The same configuration was already used in the SFTP Upload Example. After that, we have to create a MessageSource\u003cFile\u003e bean and define it as a @InboundChannelAdapter. This component is responsible for regularly checking a remote SFTP server whether a new file exists. A regular period defines annotation @Poller inside definition of InboundChannelAdapter (Poller is defined by the cron expression). Then, we need to create an instance of a SftpInboundFileSynchronizer (will be used by @InboundChannelAdapter), which defines a strategy of a synchronization mechanism, i.e. we are able to set remote filename filters (sftpRemoteDirectoryDownloadFilter), a remote directory path (sftpRemoteDirectoryDownload) or whether a remote file should be deleted after a successful transfer. The last important bean is related to a general MessageHandler bean used as a @ServiceActivator. The MessageHandler processes incoming files. @Configuration public class SftpConfig { @Value(\"${sftp.host}\") private String sftpHost; @Value(\"${sftp.port:22}\") private int sftpPort; @Value(\"${sftp.user}\") private String sftpUser; @Value(\"${sftp.privateKey:#{null}}\") private Resource sftpPrivateKey; @Value(\"${sftp.privateKeyPassphrase:}\") private String sftpPrivateKeyPassphrase; @Value(\"${sftp.password:#{null}}\") private String sftpPasword; @Value(\"${sftp.remote.directory.download:/}\") private String sftpRemoteDirectoryDownload; @Value(\"${sftp.local.directory.download:${java.io.tmpdir}/localDownload}\") private String sftpLocalDirectoryDownload; @Value(\"${sftp.remote.directory.download.filter:*.*}\") private String sftpRemoteDirectoryDownloadFilter; @Bean public SessionFactory\u003cLsEntry\u003e sftpSessionFactory() { DefaultSftpSessionFactory factory = new DefaultSftpSessionFactory(true); factory.setHost(sftpHost); factory.setPort(sftpPort); factory.setUser(sftpUser); if (sftpPrivateKey != null) { factory.setPrivateKey(sftpPrivateKey); factory.setPrivateKeyPassphrase(sftpPrivateKeyPassphrase); } else { factory.setPassword(sftpPasword); } factory.setAllowUnknownKeys(true); return new CachingSessionFactory\u003cLsEntry\u003e(factory); } @Bean public SftpInboundFileSynchronizer sftpInboundFileSynchronizer() { SftpInboundFileSynchronizer fileSynchronizer = new SftpInboundFileSynchronizer(sftpSessionFactory()); fileSynchronizer.setDeleteRemoteFiles(true); fileSynchronizer.setRemoteDirectory(sftpRemoteDirectoryDownload); fileSynchronizer .setFilter(new SftpSimplePatternFileListFilter(sftpRemoteDirectoryDownloadFilter)); return fileSynchronizer; } @Bean @InboundChannelAdapter(channel = \"fromSftpChannel\", poller = @Poller(cron = \"0/5 * * * * *\")) public MessageSource\u003cFile\u003e sftpMessageSource() { SftpInboundFileSynchronizingMessageSource source = new SftpInboundFileSynchronizingMessageSource( sftpInboundFileSynchronizer()); source.setLocalDirectory(new File(sftpLocalDirectoryDownload)); source.setAutoCreateLocalDirectory(true); source.setLocalFilter(new AcceptOnceFileListFilter\u003cFile\u003e()); return source; } @Bean @ServiceActivator(inputChannel = \"fromSftpChannel\") public MessageHandler resultFileHandler() { return new MessageHandler() { @Override public void handleMessage(Message\u003c?\u003e message) throws MessagingException { System.err.println(message.getPayload()); } }; } } ","date":"2017-05-22","objectID":"/spring-integration-sftp-download/:2:0","tags":null,"title":"Spring Integration: SFTP Download using Key-Based Authentication","uri":"/spring-integration-sftp-download/"},{"categories":["java"],"content":"Setup Spring Boot with Spring Integration I have used Spring Boot in my example, so annotation @SpringBootApplication is obvious. The more interesting annotation is @IntegrationComponentScan and @EnableIntegration which will enable all other configurations used in the previous configuration file. @SpringBootApplication @IntegrationComponentScan @EnableIntegration public class SpringSftpDownloadDemoApplication { public static void main(String[] args) { SpringApplication.run(SpringSftpDownloadDemoApplication.class, args); } } ","date":"2017-05-22","objectID":"/spring-integration-sftp-download/:3:0","tags":null,"title":"Spring Integration: SFTP Download using Key-Based Authentication","uri":"/spring-integration-sftp-download/"},{"categories":["java"],"content":"Example of Usage Here you can see a basic use case. I have created an integration test using a real SFTP server with enabled public key authentication (i.e. without password). This test starts an asynchronous thread to check an existence of a downloaded file. @RunWith(SpringRunner.class) @SpringBootTest @TestPropertySource(properties = { \"sftp.port = 10022\", \"sftp.remote.directory.download.filter=*.xxx\"}) public class SpringSftpDownloadDemoApplicationTests { private static EmbeddedSftpServer server; private static Path sftpFolder; @Value(\"${sftp.local.directory.download}\") private String localDirectoryDownload; @BeforeClass public static void startServer() throws Exception { server = new EmbeddedSftpServer(); server.setPort(10022); sftpFolder = Files.createTempDirectory(\"SFTP_DOWNLOAD_TEST\"); server.afterPropertiesSet(); server.setHomeFolder(sftpFolder); // Starting SFTP if (!server.isRunning()) { server.start(); } } @Before @After public void clean() throws IOException { Files.walk(Paths.get(localDirectoryDownload)).filter(Files::isRegularFile).map(Path::toFile) .forEach(File::delete); } @Test public void testDownload() throws IOException, InterruptedException, ExecutionException, TimeoutException { // Prepare phase Path tempFile = Files.createTempFile(sftpFolder, \"TEST_DOWNLOAD_\", \".xxx\"); // Run async task to wait for expected files to be downloaded to a file // system from a remote SFTP server Future\u003cBoolean\u003e future = Executors.newSingleThreadExecutor().submit(new Callable\u003cBoolean\u003e() { @Override public Boolean call() throws Exception { Path expectedFile = Paths.get(localDirectoryDownload).resolve(tempFile.getFileName()); while (!Files.exists(expectedFile)) { Thread.sleep(200); } return true; } }); // Validation phase assertTrue(future.get(10, TimeUnit.SECONDS)); assertTrue(Files.notExists(tempFile)); } @AfterClass public static void stopServer() { if (server.isRunning()) { server.stop(); } } } The source code of this project could be found on my public Github profile. ","date":"2017-05-22","objectID":"/spring-integration-sftp-download/:4:0","tags":null,"title":"Spring Integration: SFTP Download using Key-Based Authentication","uri":"/spring-integration-sftp-download/"},{"categories":["java"],"content":"This example will demonstrate how to use Spring Integration for uploading files to a remote SFTP server. You can use both of possible authentication methods, i.e. with a public key or with a password. The real example shows the public key authentication only because it is a more production-ready choice. Technologies used: Spring Boot 2.0.4.RELEASE Spring Integration 5.0.7.RELEASE (managed by Spring Boot) Spring 5.0.8.RELEASE (managed by Spring Boot) Quick overview: Create SFTP Session Factory, i.e. DefaultSftpSessionFactory Create and setup SftpMessageHandler Create UploadGateway as an entry point to upload any file ","date":"2017-04-27","objectID":"/spring-integration-sftp-upload-example/:0:0","tags":["sftp","spring","spring-boot","spring-integration"],"title":"Spring Integration: SFTP Upload Example using Key-Based Authentication","uri":"/spring-integration-sftp-upload-example/"},{"categories":["java"],"content":"Project Structure A final project directory structure. ","date":"2017-04-27","objectID":"/spring-integration-sftp-upload-example/:1:0","tags":["sftp","spring","spring-boot","spring-integration"],"title":"Spring Integration: SFTP Upload Example using Key-Based Authentication","uri":"/spring-integration-sftp-upload-example/"},{"categories":["java"],"content":"SftpConfig using Java Configuration We have to configure SFTP Session Factory (DefaultSftpSessionFactory) with all required parameters, i.e. host, IP port, username and password (or private key with a passphrase). After that, we have to configure an appropriate MessageHandler, i.e. SftpMessageHandler in our case. It is responsible for uploading any incoming file to a remote SFTP server, so we must provide a remote directory path and a filename to be used on a remote server. MessageHandler is a part of the Spring Integration, so we have to create a gateway between Spring Integration world (i.e. a world using channels, channel subscribers etc.) and the well-known world of simple beans. So the following annotation MessagingGateway on the interface UploadGateway will create a simple bean possible to be used anywhere you want to upload a file by calling upload method only. @Configuration public class SftpConfig { @Value(\"${sftp.host}\") private String sftpHost; @Value(\"${sftp.port:22}\") private int sftpPort; @Value(\"${sftp.user}\") private String sftpUser; @Value(\"${sftp.privateKey:#{null}}\") private Resource sftpPrivateKey; @Value(\"${sftp.privateKeyPassphrase:}\") private String sftpPrivateKeyPassphrase; @Value(\"${sftp.password:#{null}}\") private String sftpPasword; @Value(\"${sftp.remote.directory:/}\") private String sftpRemoteDirectory; @Bean public SessionFactory\u003cLsEntry\u003e sftpSessionFactory() { DefaultSftpSessionFactory factory = new DefaultSftpSessionFactory(true); factory.setHost(sftpHost); factory.setPort(sftpPort); factory.setUser(sftpUser); if (sftpPrivateKey != null) { factory.setPrivateKey(sftpPrivateKey); factory.setPrivateKeyPassphrase(sftpPrivateKeyPassphrase); } else { factory.setPassword(sftpPasword); } factory.setAllowUnknownKeys(true); return new CachingSessionFactory\u003cLsEntry\u003e(factory); } @Bean @ServiceActivator(inputChannel = \"toSftpChannel\") public MessageHandler handler() { SftpMessageHandler handler = new SftpMessageHandler(sftpSessionFactory()); handler.setRemoteDirectoryExpression(new LiteralExpression(sftpRemoteDirectory)); handler.setFileNameGenerator(new FileNameGenerator() { @Override public String generateFileName(Message\u003c?\u003e message) { if (message.getPayload() instanceof File) { return ((File) message.getPayload()).getName(); } else { throw new IllegalArgumentException(\"File expected as payload.\"); } } }); return handler; } @MessagingGateway public interface UploadGateway { @Gateway(requestChannel = \"toSftpChannel\") void upload(File file); } } ","date":"2017-04-27","objectID":"/spring-integration-sftp-upload-example/:2:0","tags":["sftp","spring","spring-boot","spring-integration"],"title":"Spring Integration: SFTP Upload Example using Key-Based Authentication","uri":"/spring-integration-sftp-upload-example/"},{"categories":["java"],"content":"Setup Spring Boot with Spring Integration I have used Spring Boot in my example, so annotation @SpringBootApplication is obvious. The more interesting annotation is @IntegrationComponentScan and @EnableIntegration which will enable all other configurations used in the previous configuration file. @SpringBootApplication @IntegrationComponentScan @EnableIntegration public class SpringSftpUploadDemoApplication { public static void main(String[] args) { SpringApplication.run(SpringSftpUploadDemoApplication.class, args); } } ","date":"2017-04-27","objectID":"/spring-integration-sftp-upload-example/:3:0","tags":["sftp","spring","spring-boot","spring-integration"],"title":"Spring Integration: SFTP Upload Example using Key-Based Authentication","uri":"/spring-integration-sftp-upload-example/"},{"categories":["java"],"content":"Example of Usage Here you can see a basic usage of our UploadGateway. I have created an integration test using a real SFTP server with enabled public key authentication (i.e. without password). @RunWith(SpringRunner.class) @SpringBootTest @TestPropertySource(properties = { \"sftp.port = 10022\" }) public class SpringSftpUploadDemoApplicationTests { @Autowired private UploadGateway gateway; private static EmbeddedSftpServer server; private static Path sftpFolder; @BeforeClass public static void startServer() throws Exception { server = new EmbeddedSftpServer(); server.setPort(10022); sftpFolder = Files.createTempDirectory(\"SFTP_UPLOAD_TEST\"); server.afterPropertiesSet(); server.setHomeFolder(sftpFolder); // Starting SFTP if (!server.isRunning()) { server.start(); } } @Before @After public void cleanSftpFolder() throws IOException { Files.walk(sftpFolder).filter(Files::isRegularFile).map(Path::toFile).forEach(File::delete); } @Test public void testUpload() throws IOException { // Prepare phase Path tempFile = Files.createTempFile(\"UPLOAD_TEST\", \".csv\"); // Prerequisites assertEquals(0, Files.list(sftpFolder).count()); // test phase gateway.upload(tempFile.toFile()); // Validation phase List\u003cPath\u003e paths = Files.list(sftpFolder).collect(Collectors.toList()); assertEquals(1, paths.size()); assertEquals(tempFile.getFileName(), paths.get(0).getFileName()); } @AfterClass public static void stopServer() { if (server.isRunning()) { server.stop(); } } } The source code of this project could be found on my public Github profile. ","date":"2017-04-27","objectID":"/spring-integration-sftp-upload-example/:4:0","tags":["sftp","spring","spring-boot","spring-integration"],"title":"Spring Integration: SFTP Upload Example using Key-Based Authentication","uri":"/spring-integration-sftp-upload-example/"},{"categories":["java"],"content":"This post shows how to implement parallelism in Java using its native java.util.concurrent classes. I mean especially using parallel Fork-Join Framework (available since Java 1.7), which is most suitable for processing of high complex (CPU intensive) tasks. I mean that case when you have one very complex task and you want to use all of your available computing resources to process your single task in the fastest time. The basic idea of the Fork-Join Framework concerns “Divide and Conquer”. You need to split a big task up into enough subtasks to keep all of your CPUs utilized. You have no limit how many times you subdivide a task but be aware that a subdividing adds some overhead as well. It can be a little hard to determine the number of subtasks. I will discuss it later with a benchmark example. The Fork-Join Framework offers two basic abstract tasks: RecursiveAction and RecursiveTask. The basic difference is related to its return value. RecursiveTask is appropriate when you need to return a result from your task, e.g. sorting a really huge array. A result of each subtask needs to be compared with each other. This task is a little bit harder to code. RecursiveAction does not return any result, you can use it e.g. to initialize a big array with some custom values. Each of subtask works alone on its own piece of that array. ","date":"2016-03-18","objectID":"/parallel-processing-java/:0:0","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"How to choose an appropriate big task I would like to make a benchmark with a parallel processing, so I need to choose a really CPU intensive task. The opposite - I/O intensive tasks are not clear because you may not see any performance benefits using parallelism, e.g. processing of a big file from a file system, where you can suffer from reading to slow from your disc. I’ve chosen a computation of all prime numbers up to a defined limit. This post is not about choosing the best and most efficient algorithm to check whether a number is prime or not, but I have used a little bit optimized way to check a number is prime (inspired by Javin Paul’s blog page): public static boolean isPrime(int num) { if (num == 2 || num == 3) { return true; } if (num % 2 == 0 || num % 3 == 0) { return false; } for (int i = 3; i \u003c= Math.sqrt(num); i += 2) { if (num % i == 0) { return false; } } return true; } Basically, it consists of testing whether a num variable is multiple of any integer between 2 and sqrt{n} with some improvements, e.g. skipping even numbers. ","date":"2016-03-18","objectID":"/parallel-processing-java/:1:0","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"How to create RecursiveAction In a case you want to implement a RecursiveAction, you need to create your own class which extends from java.util.concurrent.RecursiveAction (which is actually an abstract class) and implement it’s abstract method compute(): class PrimeRecursiveAction extends RecursiveAction { private int threshold; private Set\u003cInteger\u003e data; private int start; private int end; public PrimeRecursiveAction(Set\u003cInteger\u003e data, int start, int end, int threshold) { this.data = data; this.start = start; // start to process from this.end = end; // end of processing this.threshold = threshold; } @Override protected void compute() { if (end - start \u003c= threshold) { // Am I able to process it alone? // do the task for (int i = start; i \u003c end; i++) { if (PrimeNumberUtil.isPrime(i)) { data.add(i); } } } else { // split too big task int halfAmount = ((end - start) / 2) + start; PrimeRecursiveAction leftTask = new PrimeRecursiveAction(data, start, halfAmount, threshold); leftTask.fork(); // add left task to the queue PrimeRecursiveAction rightTask = new PrimeRecursiveAction(data, halfAmount, end, threshold); rightTask.compute(); // work on right task, this is a recursive call leftTask.join(); // wait for queued task to be completed } } } The following method calls are very important: compute(), fork(), and join(). ","date":"2016-03-18","objectID":"/parallel-processing-java/:2:0","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"Compute() As I said before, the main part to be coded is a decision how many levels of recursion are appropriate (i.e. the first condition including the threshold variable). The main question is: Am I able to process it alone? When you call compute() on the rightTask actually you are doing a recursive call. ","date":"2016-03-18","objectID":"/parallel-processing-java/:2:1","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"Fork() With the Fork-Join Framework, each thread in the ForkJoinPool has a queue of the tasks it is working on. The calling of fork() method places our newly created PrimeRecursiveAction in the current thread’s task queue. A key feature of the Fork-Join Framework is work (task) stealing. Stealing means that the other free threads in a ForkJoinPool (if any) may take and process these tasks. ","date":"2016-03-18","objectID":"/parallel-processing-java/:2:2","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"Join() When you call join() on the left (previously forked) task, it should be one of the last steps after calling fork() and compute(). Calling join() means that “I can’t continue unless this (left) task is done.” But calling join() is not only about waiting. The task you call join() on can still be in the queue (not stolen). In this case, the thread calling join() will execute the joined task. To simplify you can replace lines with calling fork(), compute() and join() with the following call invokeAll(rightTask, leftTask). Using that will also help you to avoid possible bugs with calling fork-compute-join methods in the correct order - just for completeness. ","date":"2016-03-18","objectID":"/parallel-processing-java/:2:3","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"How to call your own RecursiveAction You need to create a new instance of your RecursiveAction implementation (i.e. PrimeRecursiveAction in our case) and invoke it using ForkJoinPool. The class java.util.concurrent.ForkJoinPool is a highly specialized ExecutorService which takes in a case of no-arg ForkJoinPool constructor creates an instance that will use the Runtime.availableProcessors() method to determine the level of parallelism. It exists also a ForkJoinPool(int parallelism) constructor that allows you to set the number of threads that will be used to process your task. Set\u003cInteger\u003e data = new ConcurrentSkipListSet\u003cInteger\u003e(); ForkJoinPool fjPool = new ForkJoinPool() PrimeRecursiveAction action = new PrimeRecursiveAction(data, 0, 100, 5); fjPool.invoke(action); The constructor of the PrimeRecursiveAction needs an instance Set of Integers (to save prime numbers), a search interval to find prime numbers (we are looking for prime numbers from 0 to 100 now) and a threshold value. As you may have noticed, I had used a new instance of an ordered [ConcurrentSkipListSet](https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ConcurrentSkipListSet.html). This implementation is based on a ConcurrentSkipListMap and its concurrency support is required because multiple threads are adding values concurrently. When you do not need natural order sorting, you can use a Set based on a faster [ConcurrentHashMap](https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ConcurrentHashMap.html) (ConcurrentHashMap.newKeySet()). ","date":"2016-03-18","objectID":"/parallel-processing-java/:3:0","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"Performance test The decision how many levels of recursion are appropriate is one of the most important things that you need to do. You need to create enough subtasks that you keep all of your CPUs utilized all the time of the whole processing. I made a benchmark to find out the right size of the subtask, i.e. the threshold value. I mean the state when the current thread does not need to create another subtask anymore and is able to start checking whether a number is prime or not in its interval. ","date":"2016-03-18","objectID":"/parallel-processing-java/:4:0","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"The rules of the game number ONE There are many factors that can influence the appropriate threshold value. You have to take into account number of parallelism you have available. The decision about processing a task alone or creating other recursion calls including a management of the task queue consume your resources as well. When you creates too many subtasks, your task queue management consumes too many resources. On the other hand, a large threshold value does not keep utilized all of your CPUs at the same time. Some threads become free (no other tasks to process in the queue), but another thread might be still processing its big task. All of this vary on the amount of the required work, i.e. the upper limit where the application stops to search for prime numbers. So this my starting point: Available parallelism:4The upper limit of the search:30 000 000 (this value has no special meaning, I just needed a big task, but no too big :))Threshold values to test:1, 100, 10 000, 1 000 000 (i.e. 4 variants)Number of attempts for each variant:4, (actually 6, but the worst and the best result will be skipped) The Graph 1 shows my benchmark result. The x-axis is a time in seconds, the y-axis is the threshold value used in the each test. Graph 1: The test duration for each variant of the threshold value.Graph 1: The test duration for each variant of the threshold value. \" Graph 1: The test duration for each variant of the threshold value. As you can see the most appropriate threshold value is 100 in my case (4 available threads and the limit of the search equals to 30 000 000). In a case of the large threshold (1 000 000), one CPU was still processing its task, but other CPUs have already processed rest of tasks. See my CPU load, the first attempt is with threshold 1 000 000, the second one with threshold 100: Graph 2: The load of the CPU during processing with the threshold value 1 000 000 and 100.Graph 2: The load of the CPU during processing the threshold value 1 000 000 and 100. \" Graph 2: The load of the CPU during processing with the threshold value 1 000 000 and 100. The reason for the low performance in a case of large threshold values is clear. The threshold value 1 was slower than 100, but why? The most probable reason is too many recursions calls created during processing. See the following statistic: The upper limit of the searchThreshold valuesThe number of all created tasks during processing 30 000 000159 999 999 30 000 000 1001 048 57530 000 00010 0008 19130 000 0001 000 00063 ","date":"2016-03-18","objectID":"/parallel-processing-java/:4:1","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"The rules of the game number TWO In the previous test, we found that the most appropriate threshold value is 100 (in a case of the upper limit of the search 30 millions). I would like to know the gain of using parallel processing. I will test each of available levels of parallelism, i.e. 1-4 (on my PC) with the RecursiveAction. At the end, I will try to process the task without the RecursviceAction, simply without creating any subtasks, recursive calls and dividing intervals. Of cause, the last test will be processed by one thread only. So this my starting point: Number of available parallelisms to test:1-4 using ForkJoinPool and RecursiveAction, 1 using simple processing   (i.e. 5 variants)The upper limit of the search:30 000 000 (this value has no special meaning, I just needed a big task, but no too big :))Threshold value:100Number of attempts for each variant:4, (actually 6, but the worst and the best result will be skipped) Here is the result, the x-axis is a time in seconds, the y-axis is the level of parallelism used in the each test. Graph 3: The test duration for each variant of the parallelism level.Graph 3: The test duration for each variant of the parallelism level. \" Graph 3: The test duration for each variant of the parallelism level. As you can see, the benefit of higher parallelism level is obvious, but it does not mean, that the half number of available thread means twice as much time to process the same amount of work. There is another interesting fact about difference between using ForkJoinPool with RecursiveAction and using a simple processing algorithm (using a for cycle, for more information, check the code on my GitHub). I had suspected that the management about the queuing newly created tasks would lead to longer time in processing. The measured results do not show any difference between these two options. The overhead in management is probably not so huge to affect the result. ","date":"2016-03-18","objectID":"/parallel-processing-java/:4:2","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"Summary The Fork-Join Framework is very easy to implement by using ForkJoinPool and its RecursiveAction task. On the other hand, you have to consider whether your task is appropriate for parallel processing. If it is, you have to tune in the correct parameters (e.g. the threshold value, a level of parallelism etc.), otherwise your result will be worse than another simpler solution without using the Fork-Join Framework. ","date":"2016-03-18","objectID":"/parallel-processing-java/:5:0","tags":null,"title":"Parallel Processing in Java","uri":"/parallel-processing-java/"},{"categories":["java"],"content":"This post shows how to use a FilteredRowSet object. It lets you filter the number of rows that are visible in a RowSet object so that you can work with only the relevant data that you need. You may decide how you want to “filter” the data and apply that filter to a FilteredRowSet object. In other words, the FilteredRowSet object makes visible only the rows of data that fit within the limits you set. You may know a JdbcRowSet object, which always has a connection to its data source, so you can do filtering with a query to the data source (using WHERE clause which defines the filtering criteria). A FilteredRowSet object provides a way for a disconnected RowSet object for filtering without having to execute a query on the data source, consequently avoiding having to get a connection to the data source and sending queries to it. RowSet Classes in JDBC Java (source: http://javarevisited.blogspot.cz/2014/04/Connected-vs-disconnected-rowsetprovider-rowsetfactory-and-rowset-JDBC-Java.html)RowSet Classes in JDBC Java, FilteredRowSet \" RowSet Classes in JDBC Java (source: http://javarevisited.blogspot.cz/2014/04/Connected-vs-disconnected-rowsetprovider-rowsetfactory-and-rowset-JDBC-Java.html) There are two main steps to use the FilteredRowSet: Create a new filter (an implementation of Predicate interface). Use your filter (setting a filter to your FilteredRowSet). ","date":"2016-01-25","objectID":"/using-filteredrowset-simple-example/:0:0","tags":["filteredrowset","jdbc","rowset"],"title":"Using JDBC FilteredRowSet - Simple Example","uri":"/using-filteredrowset-simple-example/"},{"categories":["java"],"content":"Create a new filter You need to implement the evaluate method, which accepts a Rowset object. The documentation about this method says: This method is typically called a FilteredRowSet object internal methods (not public) that control the RowSet object’s cursor moving from row to the next. In addition, if this internal method moves the cursor onto a row that has been deleted, the internal method will continue to ove the cursor until a valid row is found. Other methods, like evaluate(Object value, int column) and evaluate(Object value, String columnName), are called when you are inserting new rows to a FilteredRowSet instance. My example tries to match any regular expression pattern, which was set during a filter initialization. If a pattern found, method returned true (= yes, we want this row). /** * Search Filter for {@link FilteredRowSet} * * @author pavel.sklenar * */ class SearchFilter implements Predicate { private Pattern pattern; public SearchFilter(String searchRegex) { if (searchRegex != null \u0026\u0026 !searchRegex.isEmpty()) { pattern = Pattern.compile(searchRegex); } } public boolean evaluate(RowSet rs) { System.out.println(\"SearchFilter.evaluate called \"); try { if (!rs.isAfterLast()) { String name = rs.getString(\"name\"); System.out.println(String.format( \"Searching for pattern '%s' in %s\", pattern.toString(), name)); Matcher matcher = pattern.matcher(name); return matcher.matches(); } else return false; } catch (Exception e) { e.printStackTrace(); return false; } } public boolean evaluate(Object value, int column) throws SQLException { throw new UnsupportedOperationException(\"Not supported yet.\"); } public boolean evaluate(Object value, String columnName) throws SQLException { throw new UnsupportedOperationException(\"Not supported yet.\"); } } ","date":"2016-01-25","objectID":"/using-filteredrowset-simple-example/:1:0","tags":["filteredrowset","jdbc","rowset"],"title":"Using JDBC FilteredRowSet - Simple Example","uri":"/using-filteredrowset-simple-example/"},{"categories":["java"],"content":"Use your filter You need to set your new instance of Predicate interface to your FilteredRowSet using method setFilter: usersRS.setFilter(new SearchFilter(\"^[A-L].*\")); The complete example of my code is bellow. To run it, you need to add an H2 library (a memory database driver) to your classpath, e.g. available here. /** * Example usage of {@link FilteredRowSet} * * @author pavel.sklenar * */ public class FilteredRowSetTest { /* * Sample names for test */ private static final String[] NAMES = { \"Bill Gates\", \"Steve Jobs\", \"Mark Zuckerberg\", \"Alan Turing\", \"Linus Torlvalds\" }; /** * The main class to run test */ public static void main(String[] args) throws Exception { Connection c = DriverManager.getConnection(\"jdbc:h2:mem:db1\", \"test\", \"test\"); // Just only prepare data for test prepareData(c); RowSetFactory rsf = RowSetProvider.newFactory(); FilteredRowSet usersRS = rsf.createFilteredRowSet(); usersRS.setCommand(\"select * from USER\"); usersRS.execute(c); usersRS.setFilter(new SearchFilter(\"^[A-L].*\")); dumpRS(usersRS); } /** * Dump {@link ResultSet} * * @param rs * input{@link ResultSet} to dump * @throws SQLException * @throws Exception */ public static void dumpRS(ResultSet rs) throws SQLException { ResultSetMetaData rsmd = rs.getMetaData(); int cc = rsmd.getColumnCount(); while (rs.next()) { for (int i = 1; i \u003c= cc; i++) { System.out.println(rsmd.getColumnLabel(i) + \" = \" + rs.getObject(i) + \" \"); } System.out.println(\"\"); } } /** * Prepare data for test * * @param c * {@link Connection} will be used to prepare data * @throws SQLException */ private static void prepareData(Connection c) throws SQLException { c.createStatement().execute(\"create table USER (name varchar(256))\"); PreparedStatement prepareStatement = c .prepareStatement(\"insert into USER (name) values (?)\"); for (String name : NAMES) { prepareStatement.setString(1, name); prepareStatement.execute(); } } } Output of the previous code: ContainFilter.evaluate called Searching for pattern '^[A-L].*' in Bill Gates NAME = Bill Gates ContainFilter.evaluate called Searching for pattern '^[A-L].*' in Steve Jobs ContainFilter.evaluate called Searching for pattern '^[A-L].*' in Mark Zuckerberg ContainFilter.evaluate called Searching for pattern '^[A-L].*' in Alan Turing NAME = Alan Turing ContainFilter.evaluate called Searching for pattern '^[A-L].*' in Linus Torlvalds NAME = Linus Torlvalds ","date":"2016-01-25","objectID":"/using-filteredrowset-simple-example/:2:0","tags":["filteredrowset","jdbc","rowset"],"title":"Using JDBC FilteredRowSet - Simple Example","uri":"/using-filteredrowset-simple-example/"},{"categories":["java"],"content":"Resources https://docs.oracle.com/javase/7/docs/api/javax/sql/rowset/FilteredRowSet.html https://docs.oracle.com/javase/7/docs/api/javax/sql/rowset/Predicate.html https://docs.oracle.com/javase/tutorial/jdbc/basics/filteredrowset.html http://javarevisited.blogspot.cz/2014/04/Connected-vs-disconnected-rowsetprovider-rowsetfactory-and-rowset-JDBC-Java.html ","date":"2016-01-25","objectID":"/using-filteredrowset-simple-example/:3:0","tags":["filteredrowset","jdbc","rowset"],"title":"Using JDBC FilteredRowSet - Simple Example","uri":"/using-filteredrowset-simple-example/"},{"categories":["networking"],"content":"This post shows how to make any device (running on Windows, MAC OS X, Linux-based systems) visible public (outside of your private or home network), without any public IPv4 address. I mean the state when you are stuck behind a NAT gateway or proxy and also your Internet Service Provider (ISP) did not supply any public IPv4 or IPv6 address to you. The solution is an IPv6 tunneling. Moreover, you will receive a public IP address anywhere you connect your device to any network with this tutorial, no future configuration required, isn’t it attractive for you? ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:0:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"List of Content: Why IPv6? Getting an IPv6 address Choosing a tunneling provider Tunnel setup Register your own domain Access restrictions Conclusion ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:1:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Why IPv6? IPv4 addresses become very valuable article because its global pooling capacity were already exhausted on 1 February 2011 (source). I cannot say it globally, but nowadays you may encounter problems to receive your own public IPv4 address from your Internet provider in some locations due to insufficient capacity. Available Pool of Unallocated IPv4 Internet Addresses Now Completely Emptied. The Future Rests with IPv6.(source). ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:2:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Getting an IPv6 Address So if you want to get an IPv6 address, you have few ways to do it. The first one (and the best one) is if your Internet Service Provider (ISP) supplies them to you. All you have to do is enabling IPv6 on your computer (maybe it has been already enabled) and you’re done. Most of us are not so lucky, fortunately, there exist some tunneling protocols which allows you to get and use an IPv6 address over your IPv4 connection to the internet. I do not want to use any of automatic tunneling protocols (i.e. 6to4, Teredo, ISATAP). They are not suitable for our “server” purpose. Most tunnels use IPv4 protocol 41 to transport IPv6 over IPv4 and not all firewalls and NATs can properly pass protocol 41 without any special configuration. Note: ICMP is protocol 1, IGMP is protocol 2, TCP is protocol 6, UDP is protocol 17. (source) I prefer and would like to show tunneling using tunnel brokers. These tunnels are more deterministic and easier to debug than automatic tunneling, exactly what we need to make our device public and stable all the time. You need a special broker (a company or organization providing free IPv6 in IPv4 tunneling), on the other hand, they generally support one of TSP and AYIYA protocols. These protocols send their tunneled packets over UDP, so they are passing most of NATs. ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:3:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Choosing a tunneling provider The main public providers (list of all): Hurricane Electric (6in4), Freenet6 (TSP), SixXS (6in4, L2TP or AYIYA). ProviderSupported protocolsProsConsHurricane Electric (without any personal experience)6in4Due to the used protocol, minimum packet overhead (IPv4 packet header is immediately followed by the IPv6 packet)Only 6in4 - possible problem with passing NATs (based on the protocol 41).Freenet6TSPUser-friendly setting with a desktop client.Very fast setup of a tunnel - from registration and installation to usage (compared with SixXS)Supports anonymous connections.Not so many gogo6 servers - i.e. tunnel servers  nearest to your locality (service status).Not for commercial use.SixXS6in4L2TPAYIYASupports various protocols.No traffic limits.Allowed to use SixXS commercially (without any guarantee).Client tool (AICCU) supports many platforms (Windows, Linux, FreeBSD, MAC OS X etc.)Ability to choose your nearest POP1 (many servers all around the world).Great FAQ and support on the product website.  Signup phase is verified by a real human (when you fill in e.g. a too concise reason why you want to use their service, you will be rejected and you need to repeat the whole process).Only one possible registration to one person.Signup phase followed by  a tunnel requesting phase (needs to be manually approved as well).Your latency to your POP1 must be lower than 100 ms, otherwise a tunnel will not be established. 1Point of Presence - i.e. tunnel server nearest to your locality I have some little experience with Freenet6 (which was actually fine), but I have chosen SixXS for really great support on their website and one of a POP in my country (I have very low latency around 2 ms). ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:4:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"IPv6 Tunnel setup ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:5:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Sign up for SixXS Fill in the form on the website. Be careful insufficient information provided by you may cause rejection of your request. I was rejected as well for my concise reason why I want to use tunneling. Your data will be verified by a real human so may expect your result within few days. Repeated requests take more time. ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:5:1","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Request tunnel Once when you receive your new credentials, sign in and create a new tunnel request (in the left side menu) on the User home page. Choose “Dynamic NAT-traversing IPv4 Endpoint using AYIYA” to apply for this type of tunnel (again, AYIYA has no problem to pass any NATs). ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:5:2","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Tunnel configuration Download AICCU for your platform and install a driver when needed. I want to set up my Raspberry PI to use an IPv6, so I will install AICCU to Raspbian OS (similar to Debian, Ubuntu etc.) If you are using Windows, follow these instructions. The installation process is very simple: apt-get install aiccu You will be prompt for your SixXS credentials during an installation. Once installed run aiccu start or bash/etc/init.d/aiccu start Now you can test your IPv6 connectivity to any IPv6-only website, e.g. ipv6.google.com: pi@raspberrypi / $ ping6 ipv6.google.com PING ipv6.google.com(bud02s21-in-x0e.1e100.net) 56 data bytes 64 bytes from bud02s21-in-x0e.1e100.net: icmp_seq=1 ttl=58 time=12.4 ms 64 bytes from bud02s21-in-x0e.1e100.net: icmp_seq=2 ttl=58 time=12.3 ms 64 bytes from bud02s21-in-x0e.1e100.net: icmp_seq=3 ttl=58 time=12.4 ms 64 bytes from bud02s21-in-x0e.1e100.net: icmp_seq=4 ttl=58 time=12.2 ms When you stop your tunnel: aiccu stop you may expect a negative result: pi@raspberrypi / $ ping6 ipv6.google.com connect: Network is unreachable You may test your running tunnel on http://ipv6-test.com as well: IPv6 test - IPv6_4 connectivity and speed test-IPv6 test - IPv6_4 connectivity and speed test \" IPv6 test - IPv6_4 connectivity and speed test Finally, you can test connectivity to your device from any IPv6 capable device: home@mypc / $ ping6 2a01:****:****:****::2 PING 2a01:****:****:****::2(2a01:****:****:****::2) 56 data bytes 64 bytes from 2a01:****:****:****::2: icmp_seq=1 ttl=64 time=5.1 ms 64 bytes from 2a01:****:****:****::2: icmp_seq=2 ttl=64 time=5.3 ms 64 bytes from 2a01:****:****:****::2: icmp_seq=3 ttl=64 time=5.1 ms 64 bytes from 2a01:****:****:****::2: icmp_seq=4 ttl=64 time=5.5 ms How it works: IPv6 connectivity to ipv6.google.comIPv6_tunnel \" IPv6 connectivity to ipv6.google.com ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:5:3","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Register your own domain As you noticed, your new IPv6 address is not so easy to remember as any IPv4 address. Fortunately, you can use any of domain registry provider as you are familiar with a registration of IPv4 addresses. They exist some free providers which offer registrations of third-level domains. I used to register my domains by noip.com, but they support only IPv4 domains for free (i.e. IPv6 needs AAAA records in DNS). No problem, we can try freedns.afraid.org: Adding an IPv6 capable subdomain on freedns.afraid.orgAdding a Subdomain \" Adding an IPv6 capable subdomain on freedns.afraid.org After your registration, select Subdomains (1), then select AAAA type (2) (i.e. you want to insert IPv6 address). Choose your own new third-level domain (3) and insert your IPv6 address to Destination (4). When you submit a form you need to wait a few minutes to update DNS servers. After that, your device will be accessible through the myfirstipv6domain.us.to. ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:6:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Access restrictions As you may be noticed, an IPv6 world is completely different from IPv4 and you cannot access IPv4-only sites from an IPv6-only device and vice versa. So you encounter a problem when you would like to visit your public IPv6-only device from any IPv4-only device. How to get over this limit? Get another IPv6 tunnel - fine enough, but your device will be still far away from public. The second option is more “public”. Use an IPv4 gate to translate an IPv4 request to IPv6 request. SixXS offers a free IPv4 and IPv6 gateways. ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:7:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"How to use IPv4 gateway? Check my example. You would like to visit an IPv6-only website (e.g. you have started an Apache server on your new IPv6-only device) from an IPv4-only device. We will use a domain name from the previous step, i.e. myfirstipv6domain.us.to to access our new Apache server. If so, simply append a suffix “.ipv4.sixxs.org” after your domain, i.e. myfirstipv6domain.us.to.ipv4.sixxs.org: Simple usage of an IPv4 gateway.2015-09-01 14_54_41-myrasp.us.to.ipv4.sixxs.org \" Simple usage of an IPv4 gateway. ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:7:1","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["networking"],"content":"Conclusion As you can see, it is quite easy to make any of your device public anywhere you want. Naturally, the described solution is appropriate for a home server only, where you want to share some information to public or access your home server from outside. I am using this solution on my Raspberry PI with Raspbian OS to share a simple web site from my weather station. I got rid of all dependencies on my NAT setting and port forwarding. The tunnel has no blackouts and its latency is very low: Statistic of my tunnel's latency for a whole day (source: My user profile on SixXS)statistics \" Statistic of my tunnel's latency for a whole day (source: My user profile on SixXS) ","date":"2015-10-08","objectID":"/make-your-device-public-without-any-public-ipv4/:8:0","tags":null,"title":"Make Your Device Public Without Any Public IPv4","uri":"/make-your-device-public-without-any-public-ipv4/"},{"categories":["groovy"],"content":"This post shows how to create Maven’s dependency elements (used in pom.xml file) programmatically, using DSL (Domain-Specific Language). You may find it useful, when you need to convert e.g. an old Java project (with dozens of required dependency jar files in one folder) into Maven project, i.e. create a new pom.xml file with all necessary dependency elements. Of cause, it exists a lot of different ways how to do it, but I solved this task using dynamic nature of Groovy and its metaprogramming capabilities, which makes it attractive for building DSLs. The basic idea of a domain specific language (DSL) is a computer language that’s targeted to a particular kind of problem,… M. Fowler If you are not familiar either with DSL (Domain-Specific Language) or its support in Groovy, learn from the following links: DomainSpecificLanguage by Martin Fowler Domain-Specific Languages - Groovy Doc Here is a core of our DSL implementation: def xml = new MyDependencyBuilder(out:System.out) //print dependency - using DSL xml.dependency { groupId \"org.springframework\" artifactId \"spring-core\" version \"4.2.1.RELEASE\" } The core class consist of one variable to output a result (def out) and a methodMissing implementation. Customizing the behavior of methodMissing (and propertyMissing in addition) is the heart of Groovy runtime metaprogramming, so we can create methods on the fly. Every time you call any missing method, the runtime of Groovy routes the call to our methodMissing, just before throwing a MissingMethodException exception. In a case of Closure params (line 5), we need to delegate all its calls back, to our methodMissing method using a change in Closure’s delegate property. Here is an example of usage of this class: def xml = new MyDependencyBuilder(out:System.out) //print dependency - using DSL xml.dependency { groupId \"org.springframework\" artifactId \"spring-core\" version \"4.2.1.RELEASE\" } All of the above should print something like this (without whitespaces): \u003cdependency\u003e \u003cgroupId\u003eorg.springframework\u003c/groupId\u003e \u003cartifactId\u003espring-core\u003c/artifactId\u003e \u003cversion\u003e4.2.1.RELEASE\u003c/version\u003e \u003c/dependency\u003e ","date":"2015-09-04","objectID":"/how-to-create-maven-dependencies-using-dsl-in-groovy/:0:0","tags":null,"title":"How To Create Maven Dependencies using DSL in Groovy","uri":"/how-to-create-maven-dependencies-using-dsl-in-groovy/"},{"categories":["docker","synology"],"content":"When you want to run your application in Docker on Synology you are not allowed to use all of the available parameters of the docker run command. Check my other post about basics with Docker on Synology which contains an enumeration of all possible parameters. Basically, you have two options how to run your application in Docker. Create your own original dockerfile including your application and build your new image. Use one of existing images with some well-known applications (e.g. Jenkins, Gitlab, Wordpress etc.) available on the official repository. In any case, you will need to map some network ports to your new container and/or make possible to access some resources (e.g. Shared folders) from your Synology to your new container. We will use a very simple application to demonstrate an overall deployment process. I chose GitList, which is fully available on the official hub. The overall process consists of four steps: ","date":"2015-08-08","objectID":"/how-to-run-application-in-docker-on-synology/:0:0","tags":null,"title":"How to Run Application in Docker on Synology","uri":"/how-to-run-application-in-docker-on-synology/"},{"categories":["docker","synology"],"content":"Download the GitList image to your Synology Search for the keyword “GitList” on the Registry tab in the Docker application and download it. If you need more instructions, check this. ","date":"2015-08-08","objectID":"/how-to-run-application-in-docker-on-synology/:1:0","tags":null,"title":"How to Run Application in Docker on Synology","uri":"/how-to-run-application-in-docker-on-synology/"},{"categories":["docker","synology"],"content":"Find a required docker run command You need to know a command how to run a downloaded application. You will usually find it on the official page of the downloaded image, in our case of the GitList: docker run --rm=true -p 8888:80 -v /path/repo:/repos gitlist Explaining of parameters:  --rm=true delete the current container immediately after the end of a run (useful in a case of no possible custom setting in the GitList, but Synology does not support that. -p 8888:80 GitList inside a Docker container is listening on port 80, but your Synology uses this port for another purpose. We will run our GitList on port 8888 (or whatever you want). -v /path/repo:/repos The first part (/path/repo) will be a path on your Synology and /repos is a required path by GitList inside a container. ","date":"2015-08-08","objectID":"/how-to-run-application-in-docker-on-synology/:2:0","tags":null,"title":"How to Run Application in Docker on Synology","uri":"/how-to-run-application-in-docker-on-synology/"},{"categories":["docker","synology"],"content":"Create a new container Use the downloaded image for creating a new container with your application inside the Docker application on your Synology. You may use either Launch with wizard (1) or Launch with Docker Run (2) options in the Launch menu on the Image tab: Creating containe - step 1create_container_1 \" Creating containe - step 1 There is no real difference between them, the second option tries to analyze your “docker run …” command and automatically fills in the wizard, which appears after that as well. We will use an empty wizard, set up the Container Name (1), the Local Port (2) (your choice) and the Container Port (3) (must be 80): Creating container - step 2create_container_2 \" Creating container - step 2 On Step 2 we can create a shortcut on desktop (1) in DSM to the GitList homepage: Creating container - step 3create_container_3 \" Creating container - step 3 On the Summary page, open Advanced Settings (1) to map your folder with Git repositories to GitList: Creating container - step 4create_container_4 \" Creating container - step 4 Advanced Settings gives you an ability to Add Folder (1) placed on your Synology (2) and mount it to GitList (3): Creating container - step 5create_container_5 \" Creating container - step 5 ","date":"2015-08-08","objectID":"/how-to-run-application-in-docker-on-synology/:3:0","tags":null,"title":"How to Run Application in Docker on Synology","uri":"/how-to-run-application-in-docker-on-synology/"},{"categories":["docker","synology"],"content":"Run it After you started a new container (on the Container page), you may visit GitList in your browser through the IP assigned to Synology and using the port 8888: Creating container - step 6create_container_6 \" Creating container - step 6 The Docker application is quite deeply integrated into DSM, so you are able to configure an access to your application in the Firewall setting: Creating container - step 7create_container_7 \" Creating container - step 7 NOTE: GitList itself doesn’t contain any user management or access restrictions, so enabling access to GitList outside of your network is not a good idea. ","date":"2015-08-08","objectID":"/how-to-run-application-in-docker-on-synology/:4:0","tags":null,"title":"How to Run Application in Docker on Synology","uri":"/how-to-run-application-in-docker-on-synology/"},{"categories":["docker","synology"],"content":"Installation Simply find and install a Docker application from the Synology Package Center. Note: If you do not find the application in your Package Center, your Synology is most probably not supported yet: Due to the hardware requirement, Docker will be only available on the following models: 18 series: DS3018xs, DS918+, DS718+, DS218+ 17 series: FS3017, FS2017, RS18017xs+, RS4017xs+, RS3617xs+, RS3617xs, RS3617RPxs, DS3617xs, DS1817+, DS1517+ 16 series: RS18016xs+, RS2416+, RS2416RP+, DS916+, DS716+II, DS716+, DS216+II, DS216+ 15-series: RC18015xs+, DS3615xs, DS2415+, DS1815+, DS1515+, RS815RP+, RS815+, DS415+ 14-series: RS3614xs+, RS3614xs, RS3614RPxs, RS2414RP+, RS2414+, RS814RP+, RS814+ 13-series: DS2413+, RS3413xs+, RS10613xs+, DS1813+, DS1513+, DS713+ 12-series: DS3612xs, RS3412xs, RS3412RPxs, RS2212RP+, RS2212+, DS1812+, DS1512+, RS812RP+, RS812+, DS412+, DS712+ 11-series: DS3611xs, DS2411+, RS3411xs, RS3411RPxs, RS2211RP+, RS2211+, DS1511+, DS411+II, DS411+ 10-series: DS1010+, RS810RP+, RS810+, DS710+(source, last updated 09-29-2017) ","date":"2015-07-24","objectID":"/how-to-install-and-use-docker-on-synology/:1:0","tags":["installation"],"title":"How to install and use Docker on Synology","uri":"/how-to-install-and-use-docker-on-synology/"},{"categories":["docker","synology"],"content":"How to use it? When you start a Docker application, you will see an application menu on the left side: Overview Registry Image Container Log ","date":"2015-07-24","objectID":"/how-to-install-and-use-docker-on-synology/:2:0","tags":["installation"],"title":"How to install and use Docker on Synology","uri":"/how-to-install-and-use-docker-on-synology/"},{"categories":["docker","synology"],"content":"Overview Here you will see your running containers, i.e. your running applications including allocated memory and CPU resources. Synology Docker OverviewSynology Docker Overview \" Synology Docker Overview The real command in Docker: docker ps ","date":"2015-07-24","objectID":"/how-to-install-and-use-docker-on-synology/:2:1","tags":["installation"],"title":"How to install and use Docker on Synology","uri":"/how-to-install-and-use-docker-on-synology/"},{"categories":["docker","synology"],"content":"Registry On the registry page, you can search for new images (the same as on the official site). You can also add some new repositories (in addition to the official site) in Settings. The real command in Docker: docker search ubuntu Synology Docker ImagesSynology Docker Images \" Synology Docker Images After you found your image (e.g. ubuntu in our case), you should download it to your Synology. All Images are read-only and you can use them multiple times for more containers. The real command in Docker: docker pull ubuntu ","date":"2015-07-24","objectID":"/how-to-install-and-use-docker-on-synology/:2:2","tags":["installation"],"title":"How to install and use Docker on Synology","uri":"/how-to-install-and-use-docker-on-synology/"},{"categories":["docker","synology"],"content":"Image Here you will find images available on your Synology, ready to create new containers using a wizard or directly with a docker run command. You can usually find this command on the official page with an image. The real command in Docker: docker images Synology Docker ImageSynology Docker Image \" Synology Docker Image We use a long running process for creating a new container from a docker run command: docker run -d ubuntu /bin/sh \"while true; do echo hello world; sleep 1; done\" creating docker containercreating docker container \" creating docker container The Synology wizard checks your command for compatibility, not all docker run parameters are available for use. Docker run supports the below parameters: \"d\", \"detach\" \"e\", \"env\" \"link\" \"m\", \"memory\" \"name\" \"P\", \"publish-all\" \"p\", \"publish\" \"privileged\" \"v\", \"volume\" Docker run does not support the below parameters: \"a\", \"attach\" \"add-host\" \"c\", \"cpu-shares\" \"cap-add\" \"cap-drop\" \"cidfile\" \"cpuset\" \"device\" \"dns\" \"dns-search\" \"entrypoint\" \"env-file\" \"expose\" \"h\", \"hostname\" \"i\", \"interactive\" \"lxc-conf\" \"net\" \"restart\" \"rm\" \"security-opt\" \"sig-proxy\" \"t\", \"tty\" \"u\", \"user\" \"w\", \"workdir\" Basically, your containers need to run as a daemon on your Synology (an opposite to run an interactive shell). It makes sense, you cannot run an interactive shell in your Synology Docker application in a web browser. Back to our new ubuntu container (actually got name dummyUbuntu). The wizard offers to set more options, but we want to create a dummy container, so click Next, Next, Next. ","date":"2015-07-24","objectID":"/how-to-install-and-use-docker-on-synology/:2:3","tags":["installation"],"title":"How to install and use Docker on Synology","uri":"/how-to-install-and-use-docker-on-synology/"},{"categories":["docker","synology"],"content":"Container The dummyUbuntu container appears now on the Container page. Containercontainer \" Container The real command in Docker (prints all containers including stopped): docker ps -a The last step is waiting for us: Run it. When you double click on any container, a window with some more details about a running container appears. Container detailcontainer\u0026rsquo;s detail \" Container detail You see some information about our dummyUbuntu container and capability to Start, Stop, Restart it. The real command in Docker: docker start/stop/restart dummyUbuntu You can see other information about a running container using the top menu. Container detail-processcontainer detail-process \" Container detail-process The real command in Docker: docker top dummyUbuntu On the Log tab, you can see logs from your container (not automatically refreshed, maybe in the future with the parameter -f as available in the standard Docker client). container detail-logcontainer detail-log \" container detail-log The real command in Docker: docker logs dummyUbuntu The last tab Terminal shows output of a command used to run your container (in our case the dummy long running process: \"while true; do echo hello world; sleep 1; done\" ) container detail-terminalcontainer detail-terminal \" container detail-terminal The real command in Docker: docker attach dummyUbuntu Note: The docker attach command never starts a new instance of a shell, instead of that you will see an ongoing output of the first start command. For more alternatives, see the point 8 on page 10 Useful Docker Commands – Tips and Tricks. ","date":"2015-07-24","objectID":"/how-to-install-and-use-docker-on-synology/:2:4","tags":["installation"],"title":"How to install and use Docker on Synology","uri":"/how-to-install-and-use-docker-on-synology/"},{"categories":["groovy"],"content":"If you need to use a simple logging in your Groovy script you can use some existing and supported frameworks. Sometimes you do not want or cannot use any other dependencies in your script, so you can write your own simple logger using methodMissing implementation: /** * Possibility to log TRACE, DEBUG, INFO, WARN and ERROR levels * Simply call methods like info, debug etc. (case insensitive) * Possibility to set/change: * * logFile - location of a log file (default value:default.log) * * dateFormat - format of a date in log file(default value:dd.MM.yyyy;HH:mm:ss.SSS) * * printToConsole - whether a message should be printed to console as well (default value:false) * @author pavel.sklenar * */ class Logger { private File logFile = new File(\"default.log\") private String dateFormat = \"dd.MM.yyyy;HH:mm:ss.SSS\" private boolean printToConsole = false /** * Catch all defined logging levels, throw MissingMethodException otherwise * @param name * @param args * @return */ def methodMissing(String name, args) { def messsage = args[0] if (printToConsole) { println messsage } SimpleDateFormat formatter = new SimpleDateFormat(dateFormat) String date = formatter.format(new Date()) switch (name.toLowerCase()) { case \"trace\": logFile \u003c\u003c \"${date} TRACE ${messsage}\\n\" break case \"debug\": logFile \u003c\u003c \"${date} DEBUG ${messsage}\\n\" break case \"info\": logFile \u003c\u003c \"${date} INFO ${messsage}\\n\" break case \"warn\": logFile \u003c\u003c \"${date} WARN ${messsage}\\n\" break case \"error\": logFile \u003c\u003c \"${date} ERROR ${messsage}\\n\" break default: throw new MissingMethodException(name, delegate, args) } } } Supported log levels are TRACE, DEBUG, INFO, WARN and ERROR (otherwise an exception is thrown). You can simply call these methods (case insensitive) on the instance of the Logger class: def logger = new Logger() logger.trace \"Trace level test\" logger.debug \"Debug level test\" logger.info \"Info level test\" logger.warn \"Warn level test\" logger.error \"Error level test\" Output: 23.07.2015;18:42:53.960 TRACE Trace level test 23.07.2015;18:42:54.028 DEBUG Debug level test 23.07.2015;18:42:54.029 INFO Info level test 23.07.2015;18:42:54.030 WARN Warn level test 23.07.2015;18:42:54.031 ERROR Error level test You are able to customize your logging with these properties: logFile - a location of a log file (default value:new File(“default.log”)) dateFormat - a date format in a log file (default value:dd.MM.yyyy;HH:mm:ss.SSS) printToConsole - whether a message should be printed to a console as well, i.e. in addition to a log file (default value:false) ","date":"2015-07-23","objectID":"/simple-groovy-logger-without-any-logging-framework/:0:0","tags":null,"title":"Simple Groovy Logger Without Any Logging Framework","uri":"/simple-groovy-logger-without-any-logging-framework/"},{"categories":["docker"],"content":"UPDATED: Docker deprecates the Boot2Docker command line in favor of Docker Machine. All Tips and Tricks were updated to reflect this change. ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:0:0","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Edit etc/hosts Your Docker host on Windows is usually accessible either on the IP address 192.168.59.103 (older Docker versions) or 192.168.99.101 (newer Docker versions). As you can see, not so easy to remember. You can add a new record to your etc/hosts file (in Windows) to create a local DNS name for your Docker VM (Boot2Docker), e.g.: 192.168.59.103 docker 192.168.99.101 docker which makes your life easier, see Figure 1. Figure 1: Example of a new record in etc/hosts2015-07-10 14_17_18-GlassFish Server - Server Running \" Figure 1: Example of a new record in etc/hosts NOTE: If you do not know, how to add a new record to your etc/hosts file, check this tutorial. ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:1:0","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Configure your Docker Machine (old Boot2Docker) inside Virtual Box The installation process of Docker on Windows automatically creates a new lightweight virtual machine (VM of Boot2Docker) inside VirtualBox. If you do not know what I mean, visit my another post about Docker architecture on Windows first. The used VirtualBox is completely hidden behind Docker Machine API and you usually do not need to know anymore. Sometimes you need more e.g.: Allocate more memory or CPUs to your VM, change a network connectivity either to your Windows or to the internet, raw backup your containers, or simply know where all data related to containers are saved. Basically, you have two options. You can use the boot2docker config command to generate a configuration file which you can edit and set a lot of parameters related to the VM with Boot2Docker - no more available via Docker Machine. The Docker VM is a normal VM created inside VirtualBox and you are able to change all its settings in the Oracle VM VirtualBox Manager (as I said, memory, CPUs, Network, Storages etc.), see Figure 2. Figure 2: VirtualBox interface2015-07-10 13_33_12-Oracle VM VirtualBox Správce \" Figure 2: VirtualBox interface ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:2:0","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Access Windows directories directly from any Docker container Users on Windows have one more virtualization layer (VirtualBox) between their running containers inside Docker VM (Boot2Docker) and Windows. So you cannot simply mount your Windows directory to your container inside Docker. The required setting consists of two steps: Mount a Windows directory as a folder inside Docker VM Mount that folder to any container ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:3:0","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Mount a Windows directory as a folder inside Docker VM At first you need to create a new Shared Folder in the Virtual Box setting. See Figure 3. igure 3: Shared Folders setting in Virtual Box2015-07-10 14_38_38-boot2docker-vm - Settings \" igure 3: Shared Folders setting in Virtual Box Then you need to mount this folder inside Docker VM with this command: $ mount -t vboxsf -o uid=1000,gid=50 your-shared-folder-name /existing/location/in/docker/VM In our case, the command looks like this: $ mount -t vboxsf -o uid=1000,gid=50 docker /home/docker/data NOTE: The data folder needs to exist before any mounting, i.e. call before: mkdir -p /home/docker/data. TIP: If you add this mount command to a profile file (see the following trick), your Windows directories will be accessible automatically after startup inside of your Docker VM. ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:3:1","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Mount that folder to any container This task could be done with a standard volume parameter -v from the docker run command. docker run -it -v /home/docker/data:/data ubuntu bash Now my Windows directory (D:\\development\\docker) is accessible directly inside a new ubuntu container. ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:3:2","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Permanent changes inside Docker VM The Docker VM itself is a read-only image used to boot your VM. All permanent data are stored in a Virtual Machine Disk connected and mounted to your VM (see Figure 1 and the yellow box). Basically, there are two main folders: /mnt/sda1/var/lib/boot2docker /mnt/sda1/var/lib/docker The first one contains permanent data related to Docker VM (e.g. a configuration of docker’s profile, ssh setting etc.) The second one is related to your downloaded/created images, containers etc. The boot2docker folder contains a specific file with name profile where you can add new entries to be run immediately after any system boot before the daemon starts, e.g.: export HTTP_PROXY=http://ip:port export HTTPS_PROXY=http://ip:port mkdir -p /home/docker/data mount -t vboxsf -o uid=1000,gid=50 docker /home/docker/data ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:4:0","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Connect to Docker VM via SSH You have two options how to control your Docker on Windows: Using a Windows Docker client, i.e. using a Windows Command Line Prompt (cmd.exe) or using PowerShell (powershell.exe), using a native Docker client inside your Docker VM. I prefer the second option because I used to use a Linux shell. ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:5:0","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"How to connect to your Docker VM with SSH? Your Docker VM is listening on port 22, so you can directly connect to your running VM via SSH (using username: docker, password: tcuser). If you want to use your public key without any password entry, add your public key to .ssh/authorized_keys. NOTE: You should not edit directly this file inside docker’s home folder (/home/docker/.ssh/authorized_keys). The whole docker’s home folder is not saved on the permanent storage and will be deleted during a next restart. Instead of that, edit authorized_keys file inside an existing archive userdata.tar (located in /mnt/sda1/var/lib/boot2docker/) which will be automatically untared into docker’s home directory during each startup. ","date":"2015-07-15","objectID":"/5-useful-docker-tip-and-tricks-on-windows/:5:1","tags":["commands","docker","windows"],"title":"5 Useful Docker Tips and Tricks on Windows","uri":"/5-useful-docker-tip-and-tricks-on-windows/"},{"categories":["docker"],"content":"Set HTTP proxy in your Dockerfile Your Dockerfile usually starts most probably like this: FROMtifayuki/java:8MAINTAINER...RUN apt-get update \\ wget download.java.net/glassfish/4.0/release/glassfish-4.0.zip \\ ...At first, you are apt-getting some missing applications and preparing your environment. Sometimes some applications need to be downloaded from the internet. You may encounter a problem when you are behind a proxy. Fortunately, you can use an [ENV](https://docs.docker.com/reference/builder/) command to set a HTTP/HTTPS proxy address, which will be in the environment of all “descendent” Dockerfile commands. So the updated Dockerfile example: FROMtifayuki/java:8MAINTAINER...ENV http_proxy http://server:portENV https_proxy http://server:port#... some other online commands","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:1:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Mount one folder into multiple locations The Docker architecture is based on union mount, namely UnionFS, which gives you an ability to mount volumes (folders) to any location in your image, let’s see the following example (using this image tutum/glassfish): docker run -d -p 4848:4848 -p 8080:8080 -p 8181:8181 -v /data/output/gf-logs:/opt/glassfish4/glassfish/domains/domain1/logs -v /data/output/gf-logs:/var/log tutum/glassfish See both of -v params, they are mounting the same folder (/data/output/gf-logs) into exactly two existing locations (Glassfish server log and /var/log/). Finally, you have only one folder with all logs from your Glassfish. ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:2:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Long-running worker process If you are new to the docker environment, you may be surprised, that Docker containers only run as long as the command you specify is active. E.g. this ubuntu instance immediately exits after echoing “Hello World”: docker run ubuntu /bin/echo 'Hello world' If you want to create a container that runs as a daemon (like most of the applications), you need to specify a long-running command, e.g. the command that echoes “hello world” forever: docker run -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\" ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:3:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Lists of all existing containers (not only running) $ docker ps -a ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:4:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Stop all running containers docker stop $(docker ps -a -q) ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:5:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Delete all existing containers docker rm $(docker ps -a -q) If some containers are still running as a daemon, use -f (force) param immediately after rm command. ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:6:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Delete all existing images docker rmi $(docker images -q -a) ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:7:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Attach to a running container Docker Command Line API has a special command to attach to a running container. This command is not applicable in all cases, especially if a docker container has been already started using /bin/bash command, e.g. already known long running process: docker run -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\" Trying to use attach command does not start a new instance of a shell, instead of that, you will see an ongoing output of the first command, i.e.“hello world”. Since Docker version 1.3 you are able to create a new instance of a shell using exec command: sudo docker exec -i -t 878978547 bash ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:8:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Using Docker logs command In a case of running a Docker container as a daemon (docker run -d …) it may be useful to know what appears on the console output of the running container. The docker logs retrieves logs present at the time of execution. You need to run this command repeatedly to see the actual output. This is not good enough. Fortunately, it exists a helpful param -f, --follow, which follows log output and continues streaming the new output from the container’s STDOUT and STDERR.: docker logs -f 878978547 NOTE: this command is available only for containers with json-file logging driver. ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:9:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"},{"categories":["docker"],"content":"Replace CMD command in a Dockerfile You should know, that it exists two similar commands in a Dockerfile, i.e. CMD and RUN. RUN actually runs a command and commits the result at build time. The CMD instruction should be used in a Dockerfile only once to run the software contained by your image at runtime. (mostly the last command in a Dockerfile). The main purpose of a CMD is to provide defaults for an executing container. (source) You are able to replace a default CMD command from a Dockerfile, e.g. you need to customize this command: docker run myImage \"{CMD} -extraParam\" Again, CMD instruction in a Dockerfile is overwritten with your command from the command line. ","date":"2015-07-10","objectID":"/10-useful-docker-commands-tip-tricks/:10:0","tags":["commands","docker"],"title":"10 Useful Docker Commands - Tips and Tricks","uri":"/10-useful-docker-commands-tip-tricks/"}]